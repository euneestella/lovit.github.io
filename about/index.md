---
layout: page
---

.

# Education

- 서울대학교 공과대학 산업공학과 박사 (2013.03 ~ 2019.08)

졸업논문, [미등록단어 문제와 데이터 부족 현상을 해결하기 위한 비지도학습 토크나이저와 추출 기반 문서 요약 기법 (Unsupervised Korean Tokenizer and Extractive Document Summarization to Solve Out-of-Vocabulary and Dearth of Data)][phd_thesis]에서 한국어 텍스트의 토크나이징 과정에서 발생하는 미등록단어 문제를 해결하는 방법 및 학습데이터가 존재하지 않는 문서 집합 내에서 키워드를 추출하기 위한 방법들을 제안하였습니다.
이 논문은 각각 (1) 교착어 성질을 지니는 한국어에 적합한 비지도기반 토크나이저, (2) 어절 구조의 특징을 이용한 명사 추출기, (3) 문서 집합의 주제가 단일할 때 토크나이징 과정 없이 키워드와 핵심 문장을 추출하는 방법, (4) 문서 집합의 주제가 다양할 때 문서 군집화와 군집 레이블링 기법을 이용하여 키워드를 추출하는 방법, (5) 뉴스처럼 시계열 형식으로 문서가 생성될 때, 데이터 기반으로 동일 주제 구간을 탐색한 뒤 구간별로 키워드를 추출하는 방법으로 구성되어 있습니다.

- 서울대학교 공과대학 산업공학과 석사 (2011.03 ~ 2013.02)

졸업논문, 단어 추출과 스트링 매치를 이용한 수기 입력 텍스트의 노이즈 처리 (Cleansing noisy text using corpus extraction and string match)은 데이터베이스 테이블 내 입력된 제품명처럼 문장이 아닌 단어열로 이뤄진 텍스트에서 오탈자 교정 및 미등록단어 문제를 해결하는 연구입니다.
이 논문에서 음절 단위의 n-gram 을 이용하여 데이터 기반으로 단어를 추출한 뒤, 보강된 사전과 단어 간 거리 (string distance) 척도를 이용하여 오탈자를 교정하는 방법을 제안하였습니다.

- 서울대학교 공과대학 산업공학과 학사 (2005.03 ~ 2011.02)
- 서울대학교 공과대학 기술경영협동과정 학사, Double Major (2010.03 ~ 2011.02)

[phd_thesis]: https://github.com/lovit/dissertation/blob/master/dissertation.pdf


# Who am I

## 연구 내용을 중심으로

저는 텍스트를 위주로 데이터 분석을 하는 분석가 입니다.
데이터 분석을 통하여 사회의 문제를 해결하는데 기여하고 더 나은 품질의 서비스를 만드는 것에 관심이 많습니다.
이는 가장 오랫동안 수행한 일이며, 앞으로도 계속 하고 싶은 일입니다.

데이터 분석을 시작하게 된 계기는 검색의 불편함에 대한 경험 때문이었습니다.
2010 년도에 필름 사진 작업이나 인디밴드 음악에 관심이 많았습니다.
그 중에서도 제가 알고 싶은 정보들은 많은 이들이 찾는 정보가 아니었기 때문에 관련된 웹문서까지 도달하기 위해서는 질의어들을 바꿔가며 여러 번의 검색을 시도 해야했습니다.
그리고 유용한 웹페이지를 찾기 위하여 불필요한 결과들을 직접 훑으며 탐색을 해야 했습니다.
제게 유용한 정보들이 검색 결과의 5 번째 혹은 10 번째 페이지에서 발견됨을 보면서 "검색 시스템을 개선하여 정보에 대한 접근성을 높이는 일"을 하고 싶다는 생각을 했습니다.
그 일은 머신러닝을 통하여 하는 것임을 알게되어 텍스트 데이터를 다루는 방법과 머신러닝을 공부하러 대학원으로 진학하였습니다.

대학원 시절 관심을 가지던 연구분야들은 "당시에 고민하던 문제를 해결하기 위한 해법들"이었습니다.
검색 시스템에서 느낀 불편함을 개선하려면 검색 엔진의 랭킹과 추천 엔진을 다룰 수 있어야 함을 알게 되었습니다.
그래서 텍스트 데이터를 다루는 방법과 추천 및 검색에 이용되는 방법들을 공부하였습니다.
당시 추천 분야는 대부분 Collaborative Filtering (CF) 이나 SimRank 와 같은 그래프 랭킹 방법을 이용하였습니다.
그들은 직관적인 원리를 기반으로 작동하였지만, 벡터 공간에서든지 그래프에서든지 모두 최인접이웃을 찾기 위한 검색 비용 문제를 지니고 있었습니다.
실제로 사용할 수 있는 시스템까지 만드는 것이 목표였기 때문에 계산 비용 문제는 반드시 해결해야 할 문제였고, 이는 최인접이웃 검색을 위한 Approximated Nearest Neighbor Search (ANNS) 분야를 공부하게 된 계기가 되었습니다.
또한 ANNS 는 벡터 거리를 기반으로 작동하는 군집화 기법 등의 속도 개선에도 적용할 수 있음을 배웠습니다.
그때부터 머신러닝을 공부할 때 반드시 계산 비용에 대하여 고민하는 습관을 들였습니다.
그 결과 학회에서의 튜토리얼 진행, 오탈자 교정을 위한 단어 검색용 인덱서, k-means 의 계산 속도 개선과 같은 결과물을 얻을 수 있었습니다.

그러던 중 Geoffrey Hinton 의 Semantic Hashing (2007) 과 Tomas Mikolov 의 Word2Vec (2013) 에 대하여 알게 되었습니다.
벡터 공간에서 정의되는 머신러닝 알고리즘의 성능은 결국 "좋은 벡터 표현 (representation)"에 의해 결정됨을 깨닫게 되었습니다.
그 뒤 벡터 표현 방법으로써 문서에 대한 토픽 모델링, 2차원 데이터 시각화, 혹은 다양한 종류의 데이터 벡터화 방법에 대하여 공부하였습니다.
그때부터 알고리즘을 공부할 때 모델이 "어떤 정보를 어떤 벡터 형태로 학습하는지"부터 살펴봅니다.

자연어처리는 머신러닝 공부를 시작할 때부터 지금까지 꾸준히 연구하는 분야입니다.
검색이나 추천 엔진을 만들기 위하여 문서를 벡터로 표현해야 했는데, 당시에 토크나이징에 이용하던 꼬꼬마 형태소 분석기는 미등록단어 문제가 빈번하게 발생하였습니다.
저는 이를 해결하기 위하여, "사람처럼 언어의 학습과 문서의 처리를 동시에 수행하는 한국어 토크나이저"를 만들기로 결심했습니다.
사람은 새로운 도메인의 문서라 하더라도 손쉽게 새로운 단어를 파악할 수 있습니다.
이는 사람은 문서 내 여러 문장들을 복합적으로 이용하여 문서 집합을 이해하기 때문입니다.
이 과정에는 미등록단어의 이해 및 의미의 유추 과정도 포함되어 있습니다.
토크나이저 역시 이처럼 처리하는 문서에 대한 적응 능력이 있어야 한다고 생각합니다.
그때부터 하나의 문서나 한 사건에 대한 뉴스들과 같은 동일 주제의 문서 집합으로부터 스스로 단어를 학습하여 문장을 토크나이징 하는 방법에 대하여 연구하기 시작했습니다.
그 과정에서 한국어를 대상으로 Word Piece Model 보다 단어 추출 능력이 좋은 통계 기반 토크나이저와 명사 추출 방법들을 제안하였으며, 이와 관련된 연구들을 정리하여 박사학위과정을 마쳤습니다.
연구 도중 완성된 함수들을 모아 soynlp 라는 파이썬 패키지로 정리하였습니다.

당분간 **"한국어 처리를 위하여 토크나이저를 개선하거나 임베딩 벡터 학습법을 연구하는 일"을 하고 싶습니다.**
처음에 기획한 스펙의 토크나이저를 완성하고 싶습니다.
토픽 모델링이나 키워드 추출 같은 단어 수준의 분석을 위해서 품사 판별과 활용된 용언의 원형 복원 기능까지 제공하는 토크나이저를 연구 중입니다.
이 과정에는 명사 뿐 아니라 다른 품사의 형태소들을 인식하는 방법도 포함될 예정입니다.
특히 대화체에는 입력의 편의성과 어투 때문에 예상치 못하는 다양한 종류의 어미와 부사들이 등장합니다.
이를 처리하기 위해서는 위의 기능이 반드시 제공되야 한다고 생각합니다.
더하여 이러한 토크나이저가 학습데이터에 대한 의존성이 작다면, 특수한 단어들이 주로 이용되는 도메인에서도 특별한 튜닝 과정 없이 범용적으로 이용될 것이라 기대합니다.

더하여 최근에는 토크나이징 과정을 거치지 않으면서 새로운 단어와 이들의 임베딩 벡터를 학습하는 방법에 대해서도 연구 중입니다.
BERT 와 같은 임베딩 기반 자연어처리 모델들은 단어의 임베딩 벡터를 입력값으로 이용하기 때문에, 기존에 미등록단어로 인식되던 단어들에 대해서 좋은 품질의 임베딩 벡터를 확장할 수 있다면 이를 이용하는 과업들의 성능 개선에 도움이 될 것이라 생각합니다.
또한 객체명인식과 같은 의미 수준의 단어 태깅 문제에서도 학습데이터의 의존도를 낮춰줄 것으로 기대하고 있습니다.

"과거의 가치는 현재의 활동이 결정한다"고 생각합니다.
대학원 시절 필요에 의해 다양한 분야를 공부하였지만, 한 분야에서 논문과 같은 학술 활동이나 이를 이용한 결과물들이 적어 조바심이 나기도 했습니다.
그러나 지금은 다양한 머신러닝 분야들을 살펴본 경험 덕분에 알고리즘에 내재된 정보의 벡터 표현법에 대하여 배울 수 있었다 생각하며, 다양한 관점으로 한가지 문제나 알고리즘을 살펴보는 습관을 들이게 되었다고 생각합니다.
이전에 했던 공부들은 결국 언젠가 한 번은 했을 공부이기에 돌아왔단 생각은 하지 않습니다.

저의 연구 분야에 대한 관심은 당시의 관심있는 문제를 해결하기 위한 방법론 혹은 구현체의 필요에 의하여 바뀌었습니다.
이는 저의 필요가 연구 및 개발의 동기였다는 뜻입니다.
"내가 쓰기 위하여 개발하는 도구"가 가장 완성도가 높다고 생각합니다.
저는 도구를 개발하거나 새로운 방법을 제안하여 주어진 문제를 해결하는 과정을 즐깁니다.
조금이라도 문제가 개선되는 모습을 확인하는 그 순간이 가장 즐겁습니다.
그것이 머신러닝 분야를 오랫동안 공부할 수 있게, 그리고 앞으로도 이를 이용한 데이터 분석 일을 하고 싶도록 만든 근본적인 동기입니다.
처음 머신러닝 분야의 공부를 시작할 때 세웠던 "정보의 접근성을 높이는 일"이라는 추상적인 목표는 "한국어 처리를 위한 토크나이저 개선과 임베딩 벡터 학습법의 연구"라는 구체적인 목표로 변경되었습니다.
지금 고민하는 문제 역시 정확한 텍스트 데이터 처리를 통하여 분석가들의 정보의 접근성을 향상시킨다고 생각합니다.

## 수행한 일들을 중심으로

삶의 태도와 목적에 영향을 준 일들이 있습니다.
그 중 하나는 강의를 진행한 경험입니다.
저는 패스트 캠퍼스에서 2017년 1월부터 현재까지 머신러닝을 이용한 텍스트 마이닝이라는 주제로 강의를 진행하고 있습니다.
좋은 기회가 생겨 박사 과정 중 강의를 시작하였습니다.
처음에는 "내가 머신러닝을 알면서 강의를 하는지", "하루 3시간의 강의를 유용한 내용들로 구성할 수 있을지"에 대해 걱정을 하였습니다.
아니나 다를까, 예상대로 처음에는 정말 힘들었습니다.
같은 내용을 반복하며 이야기함에도 불구하고 정확히 모르는 것들은 매번 설명하는 것이 힘들었습니다.
강의하려는 내용을 반복하여 공부하고 모델을 체화하기 위해 노력했습니다.
그 과정에서 "내용을 정확히 안다는 것은 간단한 말과 직관적인 그림으로 표현할 수 있는 것"이라는 기준도 세웠습니다.
이전에는 학습 결과가 좋으면 이를 당연시 받아들였는데, 이후부터는 모델의 이해를 위하여 패러매터를 해석하려는 노력들을 하였습니다.
"잘 작동하는 모델이 왜 잘 작동하는지 설명하지 못하면, 잘못 작동할 경우 그 원인을 찾을 수 없다"는 생각에서였습니다.
이해가 되지 않는다면 적어도 작동패턴이 예상될 정도까지는 모델을 경험적으로 알아야 했습니다.
이는 질문에 대한 대비이기도 했습니다.
모델이 예상과 다르게 작동할 경우 그 원인을 찾는 질문들이 가장 많았습니다.
그리고 튜토리얼 코드를 계속 수정하는 과정은 수강생 분들께 코드 리뷰를 받는 것과도 같았습니다.
일년 정도 강의를 준비하면서 얇게 알았던 지식들의 빈틈이 하나씩 메워지는 느낌을 받았습니다.

강의가 안정화되자 수강생 분들의 요구들이 보이기 시작했습니다.
그리고 그 요구들이 제게는 귀중한 배움의 기회들이었습니다.
그 중 첫째는 편의성에 대한 추구로, 잘못된 함수의 구조나 변수명은 잘못된 기대와 이해의 어려움을 야기한다는 점을 배웠습니다.
모델과 함수가 사용자의 입장에서 설계되지 않으면 이후에 개선이 어렵고, 사용자 역시 의도와 다르게 함수들을 이용함을 배웠습니다.
그때부터 실습 코드와 이전에 작성중이던 함수들을 개선하기 시작하였습니다.
둘째는 텍스트 마이닝과 관련하여 한국어로 기술된 정리된 자료들을 찾는 분들이 많다는 점이었습니다.
자료는 존재하지만, 여러 곳에 파편적으로 흩어져 있는 경우가 많았습니다.
또한 이론에 대한 내용들은 구하기 쉬우나, 정리된 코드들은 상대적으로 적다는 사실도 깨달았습니다.
더하여 실습에 이용할 수 있는 학습용 데이터는 코드보다도 더 적었습니다.
공부를 시작하는 많은 학생들이 데이터를 수집한 뒤 지쳐서 정작 공부를 그만두는 경우들을 보며 안타까웠습니다.
저는 강의를 진행하며 위의 자료들을 일정 수준 정리된 형태로 가지고 있었기 때문에 이를 공개하였습니다.
이를 통하여 머신러닝과 텍스트 마이닝을 공부하시는 분들이 조금이라도 더 효율적으로 공부하셨으면 하는 바람에서였습니다.
그리고 슬라이드 형식으로 전달하기 어려운 내용들은 블로그의 글로 정리하여 공유하기 시작하였습니다.

제가 가진 것들을 공유하자 다른 것들이 되돌아오기 시작했습니다.
질문들은 제가 미숙하게 알던 것들을 짚어줬습니다.
무엇보다도 다른 이들과 소통하기 위하여 글을 준비하다보니 설명들은 조금씩 직관적으로 변하였고, 알고리즘을 깊게 이해해야 하는 동기가 부여되었습니다.
강의 덕분에 "가장 좋은 공부법은 가르치는 것이다"는 말의 의미를 알게 되었습니다.
그러한 이유로 어느 순간부터 저는 수강생 분들에게 선생님이라 호칭합니다.
그리고 수업을 통해 배움을 얻었으니 이를 더 많은 이들과 나누고 싶다는 생각이 당연해 졌습니다.
지금은 블로그에 올릴 글을 기획, 기술, 정리하는 일련의 과정들이 제게는 취미가 되었습니다 (글을 쓰지 않아도 늘 글을 준비중입니다).

삶에 영향을 준 두번째 일은 교육봉사 동아리 활동입니다.
학부생 시절 지역사회에서 교육봉사를 2년간 하였습니다.
교내에 교육을 도와주는 봉사 단체는 여럿 있었지만, 지역사회에 필요한 것은 교육이 아닌 사람 간의 관계망이라 생각하였습니다.
우리 사회에는 여러 이유로 하루의 많은 시간을 홀로, 또는 또래들과만 보내는 청소년들이 많습니다.
그들에게 교육서비스보다 더 필요한 것은 어느 때고 연락할 수 있는 "아는 동네 형"이라 생각하였습니다.
개인적인 이유로 활동을 그만하게 되었는데 어린 친구들이 계속 걱정되었습니다.
만났던 많은 청소년들은 밝고 건강하였지만, 어른의 도움이 필요한 순간들이 있습니다.
예를 들어 갑자기 동생이 아프다거나 본인이 사고를 당했을 때, 혹은 어른들에게 중요한 의사 결정을 위한 질문들을 해야 할 때가 생길 수 있습니다.
생각을 확장해보니 그러한 순간들은 어른들이 청소년들을 사회안정망에 연결시키는 역할을 하는 것이었습니다.
경험상 청소년기에는 경찰, 구급대, 혹은 상담가 같은 사회안정망에 연락하기가 쉽지 않습니다.
그리고 우리 사회에는 다양한 사회적 약자가 함께 존재합니다.

2016년 말, 대화형 인터페이스를 공부하기 시작했습니다.
핸드폰과 같은 익숙한 디바이스에 친근한 대화형 인터페이스가 추가된다면 이들이 일부분 어른이 해야 하는 역할을 도와줄 수 있을 것이라 생각했기 때문입니다.
제게는 이것이 효율성보다도 당위성의 측면에서 중요한 문제였고, 기술로 사회의 문제를 조금이라도 개선할 수 있는 방법이라 생각했습니다.
그러나 이러한 목표는 마치 대학원 입학 당시의 "정보에 대한 접근성을 높이는 것"과 같이 추상적인 것임을 깨달았습니다.
더 구체적인 목표들이 달성되어야 하는 큰 문제였습니다.
하지만 이러한 고민의 경험을 통해서 기술로 사회의 문제를 해결, 혹은 완화하는 일의 가치를 재고하였습니다.
지금은 사회에 도움이 되는 일 중 제가 할 수 있는 것들부터 우선적으로 하는 것이 중요하다 생각합니다.

저는 교육 경험을 통하여 제가 가진 지식을 나눔으로써 발생하는 선순환을 경험하였고, 봉사 경험을 통하여 가치의 우선순위를 세울 기준을 마련하였습니다.
이러한 배움들이 지금의 저라는 사람을 만들었으며, 앞으로의 의사결정들의 기준이 될 것이라 믿습니다.


# Work experience

## 패스트 캠퍼스 "텍스트마이닝을 위한 머신러닝" 강의 (2017.1 ~ 현재)

머신러닝을 이용한 텍스트 데이터 분석을 주제로 패스트 캠퍼스에서 강의를 진행중입니다.
"가장 효율적인 공부 방법은 직접 가르치는 것이다"는 말처럼 강의는 머신러닝 기법들의 원리와 한계점들을 깊게 공부할 수 있는 기회였습니다.
같은 주제를 여러 번 강의하는 과정은 "옳다고 믿는 지식들을 점검"하는 계기가 되었으며, 질문과 요청은 부족한 부분들을 알 수 있는 리뷰였습니다.

"박사가 해야 할 일 중 하나는 한 분야의 공부를 시작하려는 사람이 효율적으로 공부할 수 있도록 지금까지의 연구를 정리하는 것"이라는 말에 공감하여, 더 많은 이들이 더 쉽게 텍스트마이닝을 공부할 수 있었으면 좋겠다는 바람에 강의자료를 정리하여 공개하였습니다.
이 자료들은 지속적으로 업데이트 중입니다.

- 강의노트: [https://github.com/lovit/textmining-tutorial][lecture_notes]
- 강의코드: [https://github.com/lovit/python_ml4nlp][lecture_python]
- 블로그: [https://lovit.github.io][blog]

[blog]: https://lovit.github.io
[lecture_notes]: https://github.com/lovit/textmining-tutorial
[lecture_python]: https://github.com/lovit/python_ml4nlp

## 노이즈가 많은 대화 데이터 분석을 위한 자연어처리 엔진 개발과 일상 대화를 위한 대화형 챗봇 엔진 개발 (2016. 06 - 2017. 03)

일상 대화 챗봇 엔진 개발을 위한 작업을 수행하였습니다.
띄어쓰기 규칙이 잘 지켜지지 않는 대화 데이터의 정제를 위하여 띄어쓰기 교정 알고리즘을 개발하였으며, 검색 기반 대화형 엔진과 이를 위한 스파스 벡터용 인덱서를 개발하였습니다.
이 과제를 통하여 정제되지 않은 텍스트 데이터의 정규화와 미등록단어 문제를 해결하는 방법에 대하여 고민하게 되었습니다.

**띄어쓰기 교정 알고리즘**을 개발하였습니다.
띄어쓰기 교정은 음절 단위의 시퀀스 레이블링으로 풀 수 있지만, Conditional Random Field 의 경우 문맥과 상관없이 이용되는 자질 (features) 때문에 공격적인 띄어쓰기 교정을 수행합니다.
이를 해결할 수 있는 보수적인 띄어쓰기 교정을 위한 휴리스틱 모델을 제안하였습니다.

- 휴리스틱 기반 띄어쓰기 교정 모델: [https://github.com/lovit/soyspacing](https://github.com/lovit/soyspacing)
- CRF 기반 띄어쓰기 교정 모델: [https://github.com/lovit/pycrfsuite_spacing](https://github.com/lovit/pycrfsuite_spacing)

**검색 기반 대화형 엔진**을 모델링 하였습니다.
답변으로 출력될 문장의 개수가 유한할 경우, 질의-답변 쌍을 이용하여 질의문장 검색 기반으로 답변을 출력할 수 있습니다.
검색 기반으로 작동하는 대화형 엔진을 구현하였습니다.

**스파스 벡터용 인덱서**를 개발하였습니다.
길이가 짧은 유사 질의 문장 검색 과업에서는 Locality Sensitive Hashing 과 같은 방법의 성능을 높이기 어려웠습니다.
이를 해결하기 위하여 역색인을 이용한 짧은 문장 검색용 인덱서를 개발하였습니다.

- 스파스 벡터용 인덱서: [https://github.com/lovit/fastcosine](https://github.com/lovit/fastcosine)

## 차량의 시계열 센서데이터의 비지도기반 구간 분리 및 주행 패턴 추출 (2015. 05 - 2016. 04)

차량은 주행 중 다양한 센서로부터 데이터를 수집합니다.
이 데이터는 차량의 상태 관리나 잘못된 주행 습관의 탐색에 이용될 수 있습니다.
한 번의 주행에는 주차 중, 시내주행, 고속주행 등 다양한 주행 구간이 포함되어 있으며, 이들을 구분하여 분석이 이뤄져야 합니다.
그러나 각 주행 구간에 대한 레이블은 존재하지 않기 때문에 시계열로 이뤄진 고차원의 센서 데이터를 동질적인 부분 구간들로 나누어야 합니다.
이를 위해서는 각 시점의 센서값 간의 거리가 정의되어야 하지만, 센서값에는 명목값과 다양한 스케일의 실수값이 섞여있기 때문에 (mixed type data) 거리를 정의하기 어렵습니다.
이를 해결하기 위하여 임베딩 기법을 이용하여 각 시점의 센서값을 벡터 공간으로 표현한 뒤, 동질적인 주행 구간을 추출하는 작업을 수행하였습니다.
이 과제에서 데이터 전처리, 모델 제안 및 개발, 결과 분석을 수행하였습니다.

## 공정로그 텍스트 데이터 분석을 위한 자연어처리 엔진 개발 및 이를 이용한 클레임 요인 분석 (2014. 12 - 2015. 03)

제품의 공정과정 중에는 다양한 테스트가 이뤄지며, 작업자들은 그 결과를 수기로 작성된 텍스트 형식의 로그로 남깁니다.
이 텍스트는 제품에 불량이 발생하였을 때 그 원인을 파악하거나 공정 과정의 이상을 탐지하는데 이용됩니다.
하지만 이러한 텍스트 데이터에는 일반 대화에서 사용되지 않는 전문용어들이 포함되어 있기 때문에 공개된 토크나이저 소프트웨어들은 잘 작동하지 않습니다.
이러한 문제를 해결하기 위해서는 데이터 기반으로 단어를 인식할 수 있는 토크나이저가 필요했습니다.
이 과제에서 음절기반 n-gram 과 Branching Entropy 를 이용하는 비지도기반 토크나이저 개발과 이를 분석코드로 연결하는 작업을 수행하였습니다.
이 과제를 통하여 한글을 차용하지만 한국어가 아닌 텍스트의 처리 방법에 대하여 고민하게 되었습니다.

## Semantic 검색을 위한 topic modeling (2013. 04 - 2013. 11)

스마트TV 에 음성으로 입력되는 "show me scary movies" 와 같은 요청에 대응하기 위하여 기존에는 컨텐츠의 장르 정보가 자주 이용되었습니다.
하지만, 장르는 그 종류가 한정적이며 모호성을 지니기 때문에 컨텐츠를 인덱싱할 수 있는 정보로 충분하지 않습니다.
이를 해결하기 토픽 모델링으로 학습된 토픽으로 영화를 인덱싱 한 뒤, 질의어와 토픽을 연결하여 사용자의 요청에 적합한 영화를 검색하였습니다.
이 과제에서 불용어 제거 및 객체명 전처리와 같은 토크나이징 작업을 수행하였습니다.

## 데이터와 분석 목적에 맞는 데이터마이닝 분석 로드맵 작성 (2013. 12 - 2014. 02)

데이터 분석을 위하여 머신러닝 알고리즘을 이용할 때에는 그 과업의 목적과 데이터의 특성에 따라 사용 가능한 분석 기법들이 다릅니다.
그러나 과업에 따른 모델의 선택은 데이터 분석의 경험이 어느 정도 쌓여야 합니다.
이를 돕기 위하여 한 분석팀이 보유한 종류의 데이터로 수행했던 분석들과 이에 사용된 알고리즘을 정리한 "분석 가이드라인"을 작성하는 과제를 수행하였습니다.
이 과제에서 사례 별 방법론 서베이 및 가이드라인 작성을 하였습니다.

## 소비자가치를 반영한 스마트TV (2012. 04 - 2012. 11)

소비자에게 유의미한 가치를 줄 수 있는 스마트TV의 기능을 기획하는 과제를 수행하였습니다.
2012년 당시는 2019년 현재처럼 VOD 서비스 시장이 발전하지 않은 상황이었기 때문에 TV 내 자체 자동녹화 기능을 기획중이었습니다.
이때 메모리 용량과 같은 제약이 있기 때문에 어떤 프로그램을 녹화할 것인지, 어느 시점에 다른 프로그램으로 대체할 것인지 의사결정을 하여야 했습니다.
이에 대한 근거를 시청률, 프로그램 리뷰 등의 데이터의 분석을 통하여 마련하는 작업을 수행하였습니다.
이 과제에서 데이터 수집, 정제 및 분석의 작업을 수행하였습니다.
과제의 목표와 별개로 여러 곳으로부터 수집된 데이터를 통합하는 과정의 어려움을 배웠습니다.
명목형으로 표현된 값이 서로 다르게 기술되거나 테이블 조인에 이용할 수 있는 키가 없는 경우, 각 값의 의미를 파악하여 행 (row) 간을 병합하는 record linkage 의 필요성을 느끼고, 이를 위한 방법들을 배웠습니다.

## 교차판매, 연쇄판매를 위한 고객 데이터 분석 (2011. 05 - 2011. 11)

기존의 고객에게 추가제품과 상위제품을 교차 혹은 연쇄판매 함으로써 기업은 이윤을 얻을 수 있습니다.
구매 가능성이 높은 대상에게 우선적으로 영업이 실행된다면 기업은 효율적으로 제품을 판매할 수 있습니다.
이를 위하여 기존 고객의 구매 데이터를 바탕으로 다른 고객의 구매 가능성을 예측하는 모델링을 수행하였습니다.
이 과제에서 수기로 입력된 텍스트 데이터의 교정을 수행하였습니다.
고객의 구매 데이터에는 수치형 값 외에도 사람이 직접 입력한 텍스트가 존재하는데, 여기에는 오자와 도메인 전문 용어들이 다수 포함되어 있어 이를 정제해야 했습니다.
이를 위하여 데이터 기반으로 단어 사전을 구축한 뒤 오탈자를 교정하는 전처리 작업을 수행하였습니다.


# Academic experience (journals, conferences, and activities)

## 데이터 기반으로 미등록단어 문제와 문맥적 모호성을 해결하는 토크나이저

문장을 단어 혹은 분석 단위의 유닛으로 분해하는 토크나이징은 텍스트 데이터를 머신러닝 알고리즘의 입력 형태로 변형하는 첫 단계입니다.
토크나이징 과정 중 미등록단어 문제나 모호성이 발생할 때, 모델은 분석하는 데이터의 패턴이 아닌 학습데이터의 패턴에 편향되는 문제가 발생합니다.
사람은 새로운 사건에 대한 뉴스나 새로운 분야의 텍스트에서 단어를 학습하고 모호성을 손쉽게 해결할 수 있습니다.
저는 학습기반 토크나이저와 사람의 능력 차이의 원인이 적용데이터에 대한 모델의 추가 학습과 각 문장의 독립적인 토크나이징에 있다고 생각합니다.
예를 들어, 새로운 사건의 뉴스에는 반드시 연속되어 등장하는 음절들, 즉 단어들이 있음에도 불구하고 이에 대한 학습을 하지 않습니다.
또한 동일 주제의 여러 문장으로부터 단어의 문맥을 파악할 수 있지만, 각 문장을 독립적으로 처리하기 때문에 문서 내 전역적인 정보를 이용하지 못합니다.
이러한 문제를 해결하기 위한 비지도기반 단어 추출 및 토크나이징 방법을 연구하고 있습니다.

- **[TK5]** *미등록단어 문제 해결을 위한 비지도학습 기반 한국어자연어처리 방법론 및 응용*, Naver TeckTalk2017
- **[TK4]** *노가다 없는 텍스트 분석을 위한 한국어 NLP*, PyconKR 2017
- **[TK3]** *통계 기반 한국어 명사 추출기*, **김현중**, 조성준, 한국BI데이터마이닝학회 2016 추계학술대회.
- **[TK2]** *Improving word segmentation with unlabeled data*, **Hyunjoong Kim**, Sungzoon Cho, and Pilsung Kang, July 2013, The 2nd International Symposium on System Informatics and Engineering (ISSIE2013), Xian, China.
- **[TK1]** *비교사학습을 이용한 한국어 단어 추출*, **김현중**, 조성준, 2013년 5월, 2013 대한산업공학회·한국경영과학회 춘계공동학술대회.
- **[TK0]** *KR-WordRank: A Korean word extraction method based on WordRank and unsupervised learning*, **Hyunjoong Kim**, Sungzoon Cho and Pilsung Kang, 대한산업공학회지, 2014, Vol. 40(1): 18-33.

## Text Normalization

블로그나 일상대화처럼 사람에 의하여 작성되는 텍스트 데이터에는 띄어쓰기 및 오탈자 등 오류가 포함되어 있으며, 이는 이를 이용하는 자연어처리 과업의 성능을 저하하는 원인입니다.
특히 한국어는 소리가 비슷한 단어는 비슷하게 해석될 뿐더러 띄어쓰기를 잘 지키지 않아도 문맥상 이해를 할 수 있기 때문에 오류의 비율이 높습니다.
오류를 교정하기 위하여 지도 (supervised) 기반 모델을 학습할 경우 모델이 적용될 데이터의 도메인이 달라질 때마다 해당 도메인에 적절한 학습데이터를 구축해야 합니다.
이러한 문제를 해결하기 위하여 학습데이터를 구축하지 않으며 띄어쓰기나 오탈자를 교정하는 연구를 수행하였습니다.

- **[TN2]** *노이즈 문서를 위한 통계 기반 한국어 띄어쓰기 교정기*, **김현중**, 황성구, 신윤하, 조한석, 김종윤, 조성준, 한국BI데이터마이닝학회 2016 추계학술대회.
- **[TN1]** *단어 추출과 스트링 매치를 이용한 수기 입력 텍스트의 노이즈 처리*, **김현중**, 조성준, 2013년 4월, 2013 한국BI데이터마이닝학회 춘계학술대회.
- **[TN0]** *클러스터링을 이용한 수기 입력 텍스트의 노이즈 처리*, **김현중**, 김동일, 정원열, 조성준, 2012년 5월, 2012 대한산업공학회·한국경영과학회 춘계공동학술대회.

## Document Clustering

문서 군집화는 대량의 문서 집합을 동일 주제의 부분집합으로 구분하는데 이용됩니다.
빠른 계산을 위하여 k-means 가 이용되지만, 문서의 개수가 클 경우 비효율성이 발생하며 군집화 학습 결과를 해석하기가 어렵습니다.
이러한 문제를 해결하기 위하여 문서의 벡터 공간의 특징을 활용한 효율적인 k-means 학습 방법과 군집 별 키워드를 추출하는 방법을 연구하였습니다.

- **[DC1]** *Improving spherical k-means for document clustering: Fast initialization, sparse centroid projection, and efficient cluster labeling* , **Hyunjoong Kim**, Han Kyul Kim, Sungzoon Cho, Expert Systems with Applications (minor review)
- **[DC0]** *문서 군집화를 위한 효율적인 k-means 활용: 빠른 학습, 군집 레이블링, 시각화*, 데이터야놀자2018

- k-means 시각화: [https://github.com/lovit/kmeans_to_pyLDAvis](https://github.com/lovit/kmeans_to_pyLDAvis)
- Spheical k-means 및 레이블링: [https://github.com/lovit/clustering4docs](https://github.com/lovit/clustering4docs)

## Topic Modeling

토픽 모델링은 문서 집합으로부터 특정 주제들을 추출하거나 문서를 주제 수준의 벡터로 표현하는데 이용됩니다.
Latent Dirichlet Allocation (LDA) 은 이를 위하여 대표적으로 이용됩니다.
그러나 생성 (generative) 모델중 하나인 LDA 는 데이터의 모든 단어에 대하여 동일한 중요도를 부여하기 때문에, LDA 의 학습 결과를 해석하기 위해서는 토픽 키워드를 탐색하는 과정이 필요합니다.
이러한 문제점을 해결하기 위하여 토픽 키워드로써 유의미한 단어들로 구성된 새로운 토픽 모델링 (TM3, TM2) 과 토픽 수준에서의 문서 벡터 표현 방법 (TM1) 에 대하여 연구하였습니다.

- **[TM3]** *Efficient topic modeling for single topic documents with clustering and lasso regression*, 신훈식, **김현중**, 조성준, 한국BI데이터마이닝학회 2017 추계학술대회.
- **[TM2]** *Finding topically relevant terms from learning distributed representation*, **김현중**, 김한결, 조성준, 한국BI데이터마이닝학회 2015 추계학술대회.
- **[TM1]** *Mapping words and documents into same topic space with neural encoder*, **김현중**, 김한결, 조성준, 한국BI데이터마이닝학회 2015 추계학술대회.
- **[TM0]** *Bridging the semantic gap in multimedia retrieval with topic extraction from user reviews search*, Eunjeong Park, **Hyunjoong Kim**, Seokho Kang, Sungzoon Cho, Yongki Lee, Jun 2014, INFORMS Conference on the Business of Big Data, San Jose, CA, USA.

- [TM2]의 구현체: [https://github.com/lovit/topic_embedding](https://github.com/lovit/topic_embedding)

## Document Representation

좋은 벡터 표현법의 조건 중 하나는 "중요한 정보가 손실되지 않아 높은 과업의 성능을 보이고, 해석력을 지니는 것"이라 생각합니다.
이러한 관점에서 문서를 벡터로 표현하는 방법을 연구하였습니다.
[DR0]은 토크나이징 과정은 문서 군집화에 큰 영향을 주지 않음을 실험적으로 관찰하고, 고속으로 문서 군집화를 할 수 있는 토크나이저를 제안한 연구입니다.
[DR1]과 [DR2]는 Doc2Vec 처럼 문서의 토픽정보는 잘 표현이 되면서 Bag-of-Words 처럼 해석이 가능한 문서의 벡터 표현 방법에 대한 연구입니다.

- **[DR2]** *Bag-of-Concepts: Comprehending Document Representation through Clustering Words in Distributed Representation*, Han Kyul Kim, **Hyunjoong Kim**, Sungzoon Cho, Neurocomputing. Volume 266, 29 November 2017, Pages 336-352
- **[DR1]** *Distributed Representation of Documents with Explicit Explanatory Features*, 김한결, **김현중**, 조성준, 한국BI데이터마이닝학회 2015 추계학술대회.
- **[DR0]** *고속 문서 군집화를 위한 의사 단어 벡터 표현*, **김현중**, 박은정, 김미숙, 김한결, 강필성, 조성준, 대한산업공학회 2015 춘계공동학술대회.

- Bag-of-Concepts 구현체: [https://github.com/lovit/bag-of-concepts](https://github.com/lovit/bag-of-concepts)

## Mixed type Data Representation

데이터가 여러 종류의 변수로 구성되어 있거나 같은 종류의 데이터가 한 공간에 모여있지 않을 경우 (linear inseparable) 데이터의 벡터 표현을 변형함으로써 각 과업의 성능을 향상할 수 있습니다.
시계열 형식의 데이터의 특징인 locality, 즉 주변 시점의 데이터로부터 현재 시점의 데이터를 예측할 수 있다는 성질을 이용하여 mixed type 형식의 데이터를 임베딩한 연구를 수행하였습니다.
현재는 일반적인 레코드 데이터에 대한 임베딩 방법을 (unsupervised mixed type data representation) 추가 연구 중입니다.

- **[MDR1]** *Representation learning for unsupervised heterogeneous multivariate time series segmentation and its application*, **Hyunjoong Kim**, Han Kyul Kim, Misuk Kim, Joosung Park, Sungzoon Cho, Keyng Bin Im, and Chang Ryeol Ryu, Computers & Industrial Engineering, 130, 272-281.
- **[MDR0]** *Embedding categorical and numerical mixed data into continuous space for measuring distance*, **김현중**, 김미숙, 김한결, 조성준, 한국BI데이터마이닝학회 2015 추계학술대회.

## Vector visualization

고차원 벡터 공간을 2 차원으로 변환하면 원 공간을 시각적으로 이해할 수 있습니다.
이는 모델 패러매터 디버깅 및 데이터 시각화 과업에 이용할 수 있습니다.
t-SNE 가 이 과업을 위해 가장 널리 이용되고 있으나, 원 공간의 동일한 지점이 다른 임베딩 값을 지니거나 원 공간의 데이터가 복잡한 구조의 분포를 지닐 경우 임베딩 결과에 왜곡이 발생합니다.
이러한 문제를 해결하기 위하여 그래프 내 유사도 (graph similarity) 방법을 이용하는 새로운 2 차원 시각화 방법을 연구하였습니다.

- **[VV0]** *Visualization of topological structure*, **김현중**, 김한결, 조성준, 한국BI데이터마이닝학회 2015 추계학술대회.


## Nearest Neighbor Search

많은 종류의 머신러닝 알고리즘은 소프트맥스, 유클리디언 거리, 코싸인 등 벡터의 내적을 기반으로 하는 함수를 이용합니다.
특히 함수 결과 값의 상위 k 개만을 이용하는 최인접이웃 탐색 문제의 경우 데이터의 개수와 검색비용이 비례합니다.
하지만 추천처럼 실시간의 검색 결과를 요구하는 경우에는 낮은 검색비용이 필수입니다.
검색의 정확도가 요구되는 상황을 위하여 iDistance 를 기반으로 하는 벡터 검색 시스템을 개발한 경험이 있으며 (NNS0, NNS1, NNS2), 고차원 벡터 공간에 적용할 수 있는 방법들을 정리하여 학회에서 튜토리얼을 발표한 경험이 있습니다 (NNS3).
또한 저차원 공간에서 근사가 아닌 정확한 최인접이웃 그래프를 구축하는 방법을 제안하였습니다 (NNS4).
최인접이웃검색을 연구하며, "알고리즘의 계산 성능을 최적화하기 위해서는 불필요한 계산을 하지 않거나, 중복된 계산을 한번만 하는 것"이라는 사실을 배웠으며, 모든 머신러닝 알고리즘을 바라볼 때 계산 효율성을 고려하게 된 계기가 되었습니다.

- **[NNS4]** *데이터마이닝 알고리즘을 위한 효율적 k-NN Graph 계산*, **김현중**, 강필성, 조성준, 2014년 11월, 대한산업공학회 2014 추계학술대회.
- **[NNS3]** *Locality Sensitive Hashing for Nearest Neighbor Problem*, **김현중**, 조성준, 2014년 11월, 한국BI데이터마이닝학회 2014 추계학술대회.
- **[NNS2]** *Fast Parameterless Ballistic Launch Point Estimation based on k-NN Search*, Soojin Kim, **Hyunjoong Kim**, Sungzoon Cho, Defence Science Journal, Volume 64, No 1, January 2014, Pages 41-47.
- **[NNS1]** *Two-phase KNN algorithm on sub-trajectory similarity search for efficient ballistic launching point estimation*, **Hyunjoong Kim**, Soojin Kim, and Sungzoon Cho, July 2012, 2012 Industrial Conference on Data Mining, Berlin, Germany.
- **[NNS0]** *2단계 부분궤적 유사도를 이용한 효율적인 탄도 궤도 및 발사지점 추정*, 김수진, **김현중**, 조성준, 2012년 5월, 2012 대한산업공학회·한국경영과학회 춘계공동학술대회.

## Recommender system

추천 시스템 기법들과 해결 중인 문제들을 정리한 서베이 연구를 진행하였으며, Personalized PageRank 를 이용한 영화 추천 엔진을 연구하였습니다.

- **[RS1]** *추천 시스템 기법 연구동향 분석*, 손지은, 김성범, **김현중**, 조성준, 대한산업공학회지, Apr 2015, Vol.41, No.2, pp.185-208.
- **[RS0]** *MovieRank: Combining Structure and Feature information Ranking Measure*, **김현중**, 허민회, 강필성, 조성준, 2013년 11월, 2013 한국BI데이터마이닝학회 추계학술대회.

## Data analytics

- *Fast novelty detection algorithm and its use in early fault detection for manufacturing process*, 고태훈, **김현중**, 강필성, 조성준, 대한산업공학회 2015 춘계공동학술대회.
- *프로그램 리뷰 사이트와 Twitter를 통한 TV 프로그램 인기도 비교*, 고태훈, 박은정, **김현중**, 강필성, 조성준, 2013년 5월, 2013 대한산업공학회·한국경영과학회 춘계공동학술대회.
- *TV프로그램 정보 기반 자동녹화 방법론 개발*, 고태훈, 박은정, **김현중**, 조성준, 정철, 윤명환, 2012년 11월, 2012 한국경영과학회 추계학술대회.
- *관객의 관람평 분석을 통한 영화의 특성 추출*, **김현중**, 조성준, 2012년 4월, 2012 한국BI데이터마이닝학회 춘계학술대회.


# Software develop experiences

## 자연어처리 패키지

**[1]** 연구, "데이터 기반으로 미등록단어 문제와 문맥적 모호성을 해결하는 토크나이저"의 주요 코드들은 **soynlp**라는 파이썬 패키지로 구현하고 있습니다.
이 패키지는 데이터 기반 단어, 명사 추출 및 이를 이용하는 토크나이징과 단어 공빈도 (co-occurrence) 행렬 계산, 텍스트 정규화 등의 유틸 함수등을 제공하며, 계속 연구와 개발을 이어가고 있습니다.

- [https://github.com/lovit/soynlp](https://github.com/lovit/soynlp)

**[2]** 연구 "[TK0]"의 구현체로, **KR-WordRank**는 동질적인 주제의 문서 집합에서 그래프 랭킹 기반으로 키워드와 핵심문장을 추출합니다.
이는 한국어 텍스트의 미등록단어 문제를 우회하여 TextRank 를 적용하는 효과가 있습니다.

- [https://github.com/lovit/KR-WordRank](https://github.com/lovit/KR-WordRank)

**[3]** 한국어의 형용사와 동사의 **표현형을 원형으로 복원**하거나 어간과 어미가 주어지면 이를 **표현형으로 활용**하는 패키지 입니다.
이는 *음절 단위의 한국어 품사 태깅에서 원형 복원* (심광섭, 2013)을 바탕으로 구현되었습니다.

- [https://github.com/lovit/korean_lemmatizer](https://github.com/lovit/korean_lemmatizer)

**[4]** 통계 기반으로 **연관어**를 탐색하거나, 문서 군집화 결과처럼 각 문서의 카테고리 정보가 주어졌을 때 **카테고리 별 키워드를 추출**하는 연구의 구현체입니다.
구현 원리는 "[DC1]"에 바탕합니다.

- [https://github.com/lovit/soykeyword](https://github.com/lovit/soykeyword)

**[5]** 문서 군집화를 위한 **Spherical k-means 및 군집 레이블링 함수**를 제공합니다.
구현 원리는 "[DC1]"에 바탕합니다.

- [https://github.com/lovit/clustering4docs](https://github.com/lovit/clustering4docs)

**[6]** 안전하게 **띄어쓰기를 교정**하는 휴리스틱 알고리즘의 구현체입니다.
구현 원리는 "[TN2]"에 바탕합니다.

- [https://github.com/lovit/soyspacing](https://github.com/lovit/soyspacing)

**[7]** 역색인을 이용하여 한국어 단어의 빠른 **수정거리 (Edit distance) 기반 검색**의 기능을 제공합니다.

- [https://github.com/lovit/inverted_index_for_hangle_editdistance](https://github.com/lovit/inverted_index_for_hangle_editdistance)

## 데이터 수집 및 관리

데이터 분석 및 공부를 하는 과정에서 느낀 점 중 하나는, "데이터는 분명 존재하지만 접근하기가 쉽지 않다"는 것이었습니다.
데이터의 접근성을 높여준다면 이를 이용하여 다양한 분석이 이뤄질 수 있고, 데이터 분석을 공부하는 많은 이들의 접근 장벽을 내려줄 것이라 믿습니다.
다음은 데이터 수집 및 편리한 사용을 위한 패키지들입니다.
어떤 데이터들은 저작권이 원제작자에게 있기 때문에 이를 수집하는 형태로 데이터의 접근성을 높이려 하였습니다.

**[1]** 세종말뭉치는 지도기반 한국어 형태소 분석기 제작에 거의 유일하게 이용할 수 있는 공개말뭉치 입니다.
하지만 데이터에 잘못된 태그가 부착, 형태소가 아닌 정보가 포함되어 있거나 잘못된 형식으로 기록되는 등의 오류가 많아 그대로 이용하기가 어렵습니다.
또한 이는 형태소 분석을 위한 말뭉치이기 때문에 복합 형태소를 하나의 단어로 인식하는 품사 판별 과업에 그대로 이용하기 어렵습니다.
이러한 문제를 개선하기 위하여 **sejong corpus cleaner** 는 오류가 존재하는 문장을 제거하여 분석이 편리한 포멧으로 세종말뭉치를 변환하거나, 품사 판별기 학습을 위하여 복합 형태소를 단일 단어로 병합하는 기능을 제공합니다.

- [https://github.com/lovit/sejong_corpus_cleaner](https://github.com/lovit/sejong_corpus_cleaner)

**[2]** 문재인 정부 시기에 청와대가 운영하는 **국민청원 게시판**을 통하여 국민들의 여론의 일부를 살펴볼 수 있습니다.
이러한 데이터를 수집한 뒤 (scraper), 청원 완료가 된 데이터를 정제하고 (archive), 이를 손쉽게 이용할 수 있도록 파이썬 형태의 패키지를 제공하는 (dataset) 작업을 수행하였습니다.

- 데이터 파일: [https://github.com/lovit/petitions_archive](https://github.com/lovit/petitions_archive)
- 데이터 패키지: [https://github.com/lovit/petitions_dataset](https://github.com/lovit/petitions_dataset)
- 데이터 수집: [https://github.com/lovit/petitions_scraper](https://github.com/lovit/petitions_scraper)

**[3]** 뉴스 기사는 특정 시간대의 사회적 이슈들에 대한 정보를 포함하고 있으며, 댓글은 당시의 여론을 파악하는데 용이합니다.
또한 특정 날짜마다 주요 주제와 작은 주제들이 혼합되어 있기 때문에 토픽 모델링이나 문서 군집화의 연구에 적합합니다.
**네이버 뉴스에서 뉴스 기사와 댓글을 수집**하는 패키지를 구현하였습니다.

- [https://github.com/lovit/naver_news_search_scraper/](https://github.com/lovit/naver_news_search_scraper/)

**[4]** 영화에 대한 리뷰 및 평점은 영화의 특성 분석에 이용할 수 있는 좋은 자료입니다.
**ImDB** 와 **네이버 영화**에서 각각 영화평과 평점을 수집하는 패키지를 구현하였습니다.

- ImDB 스크래퍼: [https://github.com/lovit/imdb_scraper](https://github.com/lovit/imdb_scraper)
- 네이버영화 스크래퍼: [https://github.com/lovit/naver_movie_scraper](https://github.com/lovit/naver_movie_scraper)



# Skills

대부분의 작업은 **파이썬**을 이용합니다.
필요한 알고리즘을 개발 혹은 구현하거나 데이터를 적절한 형태로 가공하는 일에 파이썬을 이용하고 있습니다.
딥러닝계 모델을 구현할 때에는 **PyTorch**를 이용하며, 그 외에는 최대한 파이썬과 **numpy** 만을 이용하는 편입니다.

4년 전에는 **자바**를 이용하였습니다.
복잡하지 않은 수준의 머신러닝 알고리즘은 직접 개발하여 이용하였습니다.
