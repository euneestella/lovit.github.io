---
title: Random Projection and Locality Sensitive Hashing
date: 2018-03-28 23:00:00
categories:
- machine learning
- vector indexing
tags:
- vector indexing
- lsh
---

k-nearest neighbor (k-NN) 문제는 모든 query 와 reference data 의 모든 점들 간의 거리를 계산하기 때문에 $$O(n)$$ 거리 계산 비용과 정렬 비용이 든다고 알려져 있습니다. 하지만 approximated nearest neighbor search (ANNS) 방법들은 아주 조금 정확하지 않더라도 매우 빠르게 nearest neighbors 를 찾습니다. Locality Sensitive Hashing (LSH) 는 hashing 기반 방법으로, 가장 널리 이용되는 ANNS 입니다. 더 이상 k-NN 은 $$O(n)$$ 처럼 비싼 알고리즘이 아닙니다. 

## Nearest neighbor problem

최인접이웃 (nearest neighbors)은 regression, classification 에 이용되는 가장 기본적인 모델입니다. Non-parametric regression 은 주어진 query point 와 비슷한 점들의 y 값의 평균을 이용하여 regression 을 수행합니다. 

$$y(q) = \sum_{x \in K_q} w(q, x) \times y(x)$$
where $$w(q,x) = exp \left( -d(q, x) \right)$$

Nearest neighbors 방법은 classification 에서는 Naive Bayes classifier 의 오차의 2 배 이하라고도 알려져 있으며, 가장 단순한 classifier 이기 때문에 이보다 성능이 나오지 않으면 안되는 base model 로 널리 사용됩니다. 일단 아주 직관적인 방법이기 때문에 이해도 쉽습니다.

그러나 reference data 에서 하나의 query point 와 가까운 k 개의 점을 찾기 위해서는 모든 점들과의 거리를 계산해야 합니다. Reference data 에 n 개의 점이 존재한다면 n 번의 거리계산을 해야하기 때문에 $$O(n)$$ 의 비용이 드는 매우 비싼 알고리즘이라 말합니다. 그러나 데이터의 크기가 커지면 절대 이렇게 계산하지 않습니다. 

Approximated nearest neighbor search (ANNS) 란, 정확한 k 개의 최인접이웃을 찾지는 못하더라도, 비슷한 최인접이웃을 빠르게 찾는 문제입니다. 이를 위한 방법은 다양합니다. Clustering 을 기반으로 하는 방법 (eg. i-Distance), network 기반 방법, 혹은 tree 기반 방법들도 이용되었습니다. 공통점은 벡터 공간을 단순한 공간으로 분할하는 것 (vector quantization) 입니다. 그 중에서도 가장 널리 이용된 방법은 hashing 기반 방법인 Locality Sensitive Hashing (LSH) 입니다. 


## Random Projection

Random Projection (RP) 를 이용하면 두 벡터의 거리를 보존하는 저차원의 벡터로 벡터 변환을 하는 mapper 를 만들 수 있습니다. Random projection 은 Johnson-Linderstrauss Lemma 를 이용합니다. 아래의 식을 반드시 이해할 필요는 없습니다. $$0 < \epsilon < 1$$ 일 때, $$N$$ 차원 인 $$m$$ 개의 points 가 있다면 $$N$$ 차원의 벡터를 $$n > 8 \times ln(m) / \epsilon^2$$ 인 n 차원으로 보낼 수 있는 linear mapper $$f : \mathbb{R}^N \rightarrow \mathbb{R}^n$$ 이 존재합니다. 

$$(1 - \epsilon) \rVert u - v \rVert ^2 \le \rVert f(u) - f(v) \rVert ^2 \le (1 + \epsilon) \rVert u - v \rVert ^2$$

쉽게 설명하면 두 벡터 $$u$$, $$v$$ 가 있을 때, 각각을 행렬 $$M$$를 곱하여 $$x$$, $$y$$ 를 얻습니다. $$x = Mu = f(u)$$, $$y = Mv = f(v)$$ 입니다. $$x$$, $$y$$ 의 거리는 $$u$$, $$v$$ 거리의 $$(1 - \epsilon)$$ 와 $$(1 + \epsilon)$$ 사이에 있습니다. 이렇게 만들 수 있는 행렬 $$M$$ 은 존재합니다. 어떻게 구할 수 있는지는 나중에 이야기합니다. 만약 $$u$$ 가 1만 차원이고, $$M$$ 에 의하여 100차원인 $$x$$ 를 만들 수 있다면, 거리를 보존하며 차원을 줄일 수 있는 것입니다. 

조금만 더 수학 이야기를 해보겠습니다. $$x=Mu$$, $$y=Mv$$ 라면 $$x \cdot y = u^T M^T M v$$ 입니다. 만약 $$M$$ 의 각 column 이 직교 (orthogonal) 라면 $$M^TM = I$$ 이기 때문에 $$x \cdot y = u \cdot v$$ 입니다. $$M$$ 은 내적을 보존하는 linear mapper 입니다. $$M$$ 이 orthogonal matrix 가 아니라면 $$M^T M = I + \epsilon$$ 입니다. $$\epsilon_{ij} = m_i ^T m_j$$, 즉 두 컬럼 $$i$$, $$j$$ 의 내적입니다. 그런데 Kaski (1998) 의 논문을 보면 재밌는 실험이 있습니다. $$M$$ 의 컬럼을 random unit vector 로 만듭니다. 벡터의 elements 를 random 으로 만들고, L2 norm 이 1 이 되도록 normalize 합니다. 각 random vector 의 차원이 그림의 d 입니다. Random vector 의 차원이 클수록 $$\epsilon$$ 의 절대값의 크기가 0 에 가까워집니다. 

![](https://raw.githubusercontent.com/lovit/lovit.github.io/master/_posts/figures/lsh_rp_epsilon_dist.png)

이제 직관적인 해석을 해보겠습니다. 두 개의 벡터를 random 하게 만들어 내적을 합니다. 두 벡터의 차원이 커질수록 내적의 값이 0 에 가깝다는 의미입니다. 각 벡터의 elements 를 어떤 데이터라 생각할 경우, 두 벡터의 내적은 두 벡터의 elements 간의 correlation 입니다. 내적이 0 이라는 것은 correlation 이 0 이라는 의미입니다. 두 벡터를 임의로 만들었습니다. 당연히 상관이 없죠. 그래서 내적, correlation 은 0 입니다. Random elements 10 개는 우연히도 상관이 있을 수도 있습니다. 하지만 50 개, 100 개 처럼 여러 개의 elements 를 만들수록 더더욱 상관이 없습니다.

Random Projection 은 $$M$$ 을 이처럼 random unit vector 로 만듭니다. 그럼 $$M$$ 의 컬럼이 orthogonal 에 가까워 $$x \cdot y \simeq $$u \cdot v$$ 가 됩니다. 

Bingham & Mannila (2001) 은 Random Projection 과 Principal Components Analysis (PCA) 와의 거리 보존 능력에 대한 테스트를 수행하였습니다. 그 결과는 놀랍게도 거리 보존 능력은 PCA 보다도 RP 가 좋았습니다.

![](https://raw.githubusercontent.com/lovit/lovit.github.io/master/_posts/figures/lsh_rp_pca_diff.png)

데이터 간의 pairwise distance 를 여러 번 계산해야 할 경우에는 PCA 같은 방법을 이용하여 데이터의 차원을 줄이곤 했습니다. 대표적으로 clustering 알고리즘들이 여러 번의 pairwise distance 계산을 요구합니다. Random projection 은 이때에도 이용됩니다.

## Locality Sensitve Hashing

Locality Sensitve Hashing (LSH) 은 Random Projection 을 이용하는 Approximated Nearest Neighbor Search (ANNS) 알고리즘입니다. 사실 Random Projection 을 이해하는게 가장 어렵습니다. 다른 것 보다도, $$u$$ 에 $$M$$ 을 곱하였더니 저차원 $$x$$ 를 얻었고, $$u$$, $$v$$ 의 거리는 $$x$$, $$y$$ 의 거리와 비슷하다라는 점만 기억하면 됩니다. 

RP의 특징을 이용하여 각 벡터, data point 마다 각자가 속한 지역의 주소를 부여하겠습니다. 그림의 original vector 에 $$M$$ 을 곱하여 mapped vector 를 만듭니다. Original vector 가 비슷하였다면 mapped vector 도 비슷할 것입니다. 그런데 mapped vector 는 float 입니다. 일단 소수점 버림을 하여 integer vector 로 벡터를 변환합니다. 맨 위의 벡터는 [1, 0, 0] 이라는 label 을 가집니다. 2, 3 번째 벡터도 original vector 가 비슷하기 때문에 모두 [1, 0, 0] label 을 할당받습니다. 이제 [1, 0, 0] 이 key 입니다. 같은 키를 지니는 벡터들을 한데 모아 bucket 을 만듭니다. 이 과정이 indexing 입니다. 

저는 이 과정을 "우연이 계속되면 필연"이라는 말로 비유합니다. 두 개의 original vectors 가 각각 5 차원의 integer vector 로 바뀌었다고 상상해봅니다. 첫번째 integer key 가 같다면 그럴 수 있습니다. 내적을 하여 비슷한 값을 가질 수 있는 벡터는 많으니까요. 하지만 두번째 integer key 도, 세번째 integer key 도 계속하여 같은 값을 가진다면 자연스레 두 벡터는 원래 같은 벡터 아닌가? 라는 의심을 할 수 있습니다. 그럴 경우 십중 팔구 비슷한 벡터였기 때문에 같은 integer keys 를 지니는 겁니다. (하지만 십중 팔구 이므로, 반드시 보장하는 것은 아닙니다.)

![](https://raw.githubusercontent.com/lovit/lovit.github.io/master/_posts/figures/lsh_indexing_framework.png)

이제 query 가 입력되어 비슷한 벡터들을 찾으려 합니다. Query vector 역시 동일한 mapper $$M$$을 곱하고, integer vector 변환을 합니다. Query 역시 [1, 0, 0] 의 key 를 얻었습니다. 그렇다면 bucket 에서 같은 [1, 0, 0] 의 키를 지닌 벡터들만 추려서 실제 거리를 계산합니다. 이는 마치 '시 - 구 - 동' 과 같은 구역체계에서 같은 '동'에 있는 data point 끼리만 거리를 계산하는 것과 같습니다. 대부분의 벡터들은 다른 key 를 가지고 있기 때문에 이미 거리 계산 비용을 매우 줄였습니다. Original vector 가 비슷하면 key 가 비슷할 가능성이 높습니다. 이처럼 Original space 에서의 locality 를 고려하여 key 를 만드는 hashing 이라 하여 Locality Sensitive Hashing 이라 부릅니다. 만약 [1, 0, 0] 에 k 개 보다 적은 점이 포함되어 있다면 [1, 0, 0]과 비슷한 [1, 1, 0] 에서 추가적인 최인접이웃을 보강할 수도 있습니다. 

## Hashing function and its geometrical interpretation

Key 를 만드는 함수를 hashing function 이라 합니다. 정확히는 $$g = (h_1, \cdots , h_m)$$ 으로 기술합니다. $$m$$ 차원의 key 를 만드는 함수 $$g$$ 입니다. 그리고 각 $$h$$ 는 다음 식과 같습니다. $$a_i$$ 는 original vector $$x$$ 와 같은 차원의 random unit vector 입니다. $$b_i$$ 는 $$[0, r]$$ 사이의 random number 입니다. $$x$$ 는 random unit vector 와 내적을 한 뒤 $$r$$ 로 나뉘어 집니다. 그리고 이 값에 $$[0, -1]$$ 의 값이 빼진 뒤, 올림을 통하여 integer 가 됩니다.

$$h_i(x) = \lfloor \frac{a_i ^T x - b_i}{r} \rfloor$$

$$b_i$$ 의 역할은 나중에 이야기하고, $$a_i$$ 를 기하학적으로 해석해봅니다. $$h_1$$ 에 의하여 같은 integer 를 지니는 공간이란, 평행한 선 ($$a_1$$) 에 의하여 나뉘어지는 공간이라 생각할 수 있습니다. Query point 의 최인접이웃이 원 안에 포함(actual search space) 되어 있다면, $$h_1$$ 에 의하여 같은 integer 를 지니는 공간만 탐색하면 됩니다. 원이 [-1] 공간에 포함되어 있지만, 원 외에도 위/아래로 많은 공간이 포함되어 있습니다. 그리고 $$r$$ 은 평행선의 간격입니다.

![](https://raw.githubusercontent.com/lovit/lovit.github.io/master/_posts/figures/lsh_concept_1_mapper.png)

여기에 $$h_2$$ 를 하나 더 적용합니다. Query point 는 [-1, 3] 의 key 를 지닙니다. 그리고 같은 키를 지니는 공간은 원을 포함합니다. 원 외의 공간은 매우 많이 줄어듭니다. $$a_i$$ 를 random vector 로 만들기 때문에 이들은 다양한 방향으로 뻣어있는 평면이 됩니다. 

![](https://raw.githubusercontent.com/lovit/lovit.github.io/master/_posts/figures/lsh_concept_1_mapper.png)

그러나 $$h_i$$ 를 많이 늘린다고하여 원에 가까운 bucket 이 만들어지는 것은 아닙니다. 한계가 있습니다. 적당히 만들어야 하는거죠. 즉 integer vector 의 차원이 적당히 커야 가장 효율적입니다. 이런 방식으로 LSH 는 query point 에 대하여 같은 key 를 지니는 점들만 후보로 선택함으로써 효율적으로 최인접이웃의 후보를 제공합니다. 

![](https://raw.githubusercontent.com/lovit/lovit.github.io/master/_posts/figures/lsh_concept_3_mappers.png)

$$g$$ 의 $$h$$ 에 의하여 공간이 부분 공간으로 나뉘어집니다 (partition). 

## Multi-layers 

그런데 hashing function $$g$$ 를 하나만 만들면 또 문제가 생깁니다. Query point 가 하나의 $$g$$ 로 만들어지는 partition 의 모서리에 위치할 수 있습니다. Query point 의 최인접이웃이 위치하는 공간은 원인데, 원의 일부분만 탐색할 수 있습니다. 

![](https://raw.githubusercontent.com/lovit/lovit.github.io/master/_posts/figures/lsh_multilayer_layer1.png)

이를 방지하기 위해서 여러 개의 $$g$$, 즉 여러 개의 layers (tables) 를 중첩하여 이용합니다. 어자피 모든 $$h$$ 를 random vector 로 만들기 때문에 또 다른 table, $$g_2$$ 는 아래 그림처럼 다른 방향의 격자로 공간을 나눈 buckets 이 됩니다. 

![](https://raw.githubusercontent.com/lovit/lovit.github.io/master/_posts/figures/lsh_multilayer_layer2.png)

이 둘을 겹치면 좀 더 그럴싸한 탐색공간 (최인접이웃의 후보)을 확보할 수 있습니다.

![](https://raw.githubusercontent.com/lovit/lovit.github.io/master/_posts/figures/lsh_multilayer_overlap.png)

그래서 최종적인 구조는 다음과 같습니다. $$x$$ 를 $$m$$ 차원의 integer vector 로 변환하는 $$g$$ 가 $$l$$ 개 있습니다. $$l$$ 개의 hash table 을 이용합니다. 하나의 query point 에 대하여 $$l$$ 개의 hash table 로부터 최인접이웃의 후보를 가져온 뒤, 이들로부터 최종적인 최인접이웃을 구합니다. 이때는 후보들과 query point 간의 실제 거리를 계산합니다. 

![](https://raw.githubusercontent.com/lovit/lovit.github.io/master/_posts/figures/lsh_multilayer_framework.png)

## sklearn.neighbors.LSHForest

Scikit-learn 에도 LSH 가 구현되어 있습니다. 이번에는 random data 를 만들어 성능 테스트를 해보겠습니다. 50 차원의 인공데이터 100만개를 만듧니다. 

{% highlight python %}
import numpy as np

x = np.random.random_sample((1000000, 50))
x.shape # (1000000, 50)
{% endhighlight %}

LSHForest 에는 몇 가지 arguments 옵션이 있습니다. 그 중 n_estimators 는 hash table 의 개수, 즉 $$g_i$$ 의 개수입니다. 많이 늘린다고 성능이 좋아지지는 않습니다. 언제나 적당히, ... 적당히 입니다. radius 는 $$r$$ 에 해당합니다. 이 값을 잘 조절해야 하긴 하지만, 적당히 크게 잡고 n_candidates 를 조절해도 됩니다. n_candidates 는 각 hash table 마다 계산할 최소한의 최인접후보의 개수입니다. 

{% highlight python %}
from sklearn.neighbors import LSHForest

LSHForest(
	n_estimators=10,
	radius=1.0,
	n_candidates=50)
{% endhighlight %}

Indexing 은 fit() 을 이용합니다. 100 만개의 데이터임에도 불구하고 인덱싱에 4 초가 걸리지 않습니다. 

{% highlight python %}
%%time

lsh = LSHForest(n_estimators=4)
lsh.fit(x)
# Wall time: 3.77 s
{% endhighlight %}

x 의 열 개의 점을 샘플링하여 5 개의 최인접이웃을 찾아봅니다. kneighbors() 함수를 이용하면 됩니다. 각 점들과의 거리인 dist 와 점들의 index 가 return 됩니다. 117 ms 만에 검색이 되었습니다. 

{% highlight python %}
%%time
dist, idxs = lsh.kneighbors(x[:10,], n_neighbors=5)
Wall time: 117 ms
{% endhighlight %}

## References
- Bingham, E., & Mannila, H. (2001, August). Random projection in dimensionality reduction: applications to image and text data. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 245-250). ACM.
- Kaski, S. (1998, May). Dimensionality reduction by random mapping: Fast similarity computation for clustering. In Neural Networks Proceedings, 1998. IEEE World Congress on Computational Intelligence. The 1998 IEEE International Joint Conference on (Vol. 1, pp. 413-418). IEEE.
- Indyk, P., & Motwani, R. (1998, May). Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing (pp. 604-613). ACM.