---
title: t-Stochastic Neighbor Embedding (t-SNE) for visualization
date: 2017-09-28 09:00:00
categories:
- nlp
- representation
tags:
- visualization
- embedding
---

t-Stochastic Nearest Neighbor (t-SNE) 는 vector visualization 을 위하여 자주 이용되는 알고리즘입니다. t-SNE 는 고차원의 벡터로 표현되는 데이터 간의 neighbor structure 를 보존하는 2 차원의 embedding vector 를 학습함으로써, 고차원의 데이터를 2 차원의 지도로 표현합니다. 이번 포스트는 t-SNE 의 원리에 대하여 살펴봅니다.

## Dimension reduction for visualization

고차원의 벡터를 이해하기 위하여 시각화 방법들이 이용됩니다. 대표적인 방법으로 t-SNE 라 불리는 t-Stochastic Neighbor Embedding 이 있습니다. t-SNE 는 고차원 공간에서 유사한 두 벡터가 2 차원 공간에서도 유사하도록, 원 공간에서의 점들 간 유사도를 보존하면서 차원을 축소합니다. 우리가 이해할 수 있는 공간은 2 차원 모니터 (지도) 혹은 3 차원의 공간이기 때문입니다.

![]({{ "/assets/figures/tsne_mnist.png" | absolute_url }}){: width="70%" height="70%"}

위 그림은 t-SNE 가 제안되었던 [Maaten (2008)][tsne_paper] 에서 10 개의 숫자 손글씨인 MNIST 데이터를 2 차원으로 압축하여 시각화한 그림입니다. 같은 색은 같은 숫자를 의미합니다. MNIST 는 (28, 28) 크기의 784 차원 데이터입니다. 우리가 784 차원을 상상할 수는 없지만, 이를 2 차원으로 압축하면 어떤 이미지들이 유사한지 시각적으로 이해할 수 있습니다.

최근에 깊게 연구되고 있는 딥러닝 모델들은 학습된 지식의 형태를 distributed representation 으로 저장합니다. 해석 불가능한 고차원의 dense vector 로 저장한다는 의미입니다. 대체로 이 벡터의 크기는 매우 큽니다. Sentence classification 이나 image classification 을 위한 Convolutional Neural Network (CNN) 모델이 softmax layer 에 내보내는 마지막 output 은 sentence 에 대한 distributed representation 입니다. Recurrent Neural Network (RNN) 의 hidden vector 들의 차원도 고차원입니다. 이들을 해석할 수는 없지만, 비슷한 input 이 비슷한 distributed representation 을 얻는지 확인하기 위하여 고차원 벡터의 시각화는 필요합니다.

물론 deep learning models 은 그 자체로 데이터 시각화를 할 수도 있습니다. 아래 그림은 Hinton 교수님의 [2006 년도 논문][hinton2006]에서 제안된 autoencoder 입니다. Restricted Boltzmann machines (RBM) 을 stacking 하여 모델을 구성하였습니다. 

![]({{ "/assets/figures/hinton2006_structure.png" | absolute_url }}){: width="70%" height="70%"}

약 2 만 개의 단어로 표현되는 20 News group 문서에 대해 문서 간 유사도 정보를 보존하는 2 차원 좌표를 학습하기 위하여 가장 깊은 encoder layer 의 차원을 2 까지 줄였습니다. 그리고 그 2 차원 벡터를 x, y 축으로 plotting 하면 아래와 같은 그림을 얻을 수 있습니다.

![]({{ "/assets/figures/hinton2006.png" | absolute_url }}){: width="70%" height="70%"}

그러나 위 그림처럼 정보를 지나치게 압축하면 autoencoder 의 복원력에 문제가 생길 수 있습니다. 복잡한 문제를 푸는 deep learning models 은 최소한의 공간이 확보되어야 하고, 그 과정에서 자연스레 고차원의 벡터가 발생합니다. 결국 이들을 해석하려면 다시 한 번 dimension reduction 을 하여야 합니다. 그리고 이 때 PCA 와 t-SNE 가 자주 이용됩니다. 물론 다른 알고리즘들도 이용할 수는 있지만, PCA 와 t-SNE 가 robust 한 결과를 학습하는 경우가 많습니다. 우리는 이번 포스트에서 t-SNE 의 학습 원리에 대해 살펴봅니다.


## t-Stochastic Neighbor Embedding (t-SNE)

$$p_{j \vert i} = \frac{exp(- \vert x_i - x_j \vert^2 / 2 \sigma_i^2)}{\sum_{k \neq i} exp(- \vert x_i - x_k \vert^2 / 2 \sigma_i^2)}$$

$$p_{ij} = \frac{p_{i \vert j} + p_{j \vert i}}{2n}$$

물론 $$p_{ij}$$ 를 아래와 같이 정의할 수도 있습니다. 모든 $$x_i$$ 에 대하여 동일한 $$\sigma$$ 를 이용하는 것입니다. 그런데 아래와 같은 정의를 이용하면 outliers 에 크게 휘둘릴 수 있습니다. 만약 $$x_i$$ 가 다른 점들과 동떨어져 있는 값이라면 다른 점들과의 거리 $$\vert x_i - x_j \vert^2$$ 가 매우 크게 됩니다. 이는 $$\sigma$$ 를 큰 값으로 만들기 때문에 가까운 두 점 $$x_k, x_l$$ 간의 거리로부터 유도되는 분자, $$exp(- \vert x_k - x_l \vert^2 / 2\sigma^2)$$ 를 매우 작게 

$$p_{ij} = \frac{exp(- \vert x_i - x_j \vert^2 / 2 \sigma^2)}{\sum_{k \neq l} exp(- \vert x_k - x_l \vert^2 / 2 \sigma^2)}$$

$$q_{ij} = \frac{ \left( 1 + \vert y_i - y_j \vert^2 \right)^{-1} }{\sum_{k \neq l} \left( 1 + \vert y_k - y_l \vert^2 \right)^{-1} }$$

$$\frac{\delta C}{\delta y_i} = \sum_j (p_{ij} - q_{ij})(y_i - y_j)$$

## Package: sklearn

## Comparison with other embedding algorithms 

- algorithm
    - lle, mds, isomap, TruncatedSVD, t-SNE
    - sparse matrix ? table 로 만들기
- data
    - swiss roll (easy, hard)
    - term - doc matrix

[hinton2006]: https://www.cs.toronto.edu/~hinton/science.pdf
[tsne_paper]: http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf