---
title: Word Piece Model (a.k.a sentencepiece)
date: 2018-04-02 13:00:00
categories:
- nlp
tags:
- preprocessing
- tokenization
---

토크나이징 (tokenizing) 은 문장을 토큰으로 나누는 과정입니다. 텍스트 데이터를 학습한 모델의 크기는 단어의 개수에 영향을 받습니다. 특히 Google neural machine translator (GNMT) 와 같은 Recurrent Neural Network (RNN) 기반 알고리즘들은 단어 개수에 비례하여 계산비용이 증가합니다. 그렇기 때문에 RNN 에 이용되는 vocabulary, word embedding 벡터의 종류가 제한됩니다. 하지만 vocabulary 의 개수가 제한되면 임베딩 벡터로 표현하지 못하는 단어가 생깁니다. 이 역시 미등록단어 문제입니다. RNN 기반 모델에서 이를 해결하기 위하여 Word Piece Model (WPM) 이 제안되었습니다. 단어를 한정적인 유닛 (finite subword units) 으로 표현합니다. WPM 은 언어에 상관없이 모두 적용할 수 있기 때문에 적용할 언어마다 해당 언어의 특징을 반영한 토크나이저를 만들지 않아도 됩니다. 하지만 데이터 분석에 언제나 적합한 것은 아닙니다. WPM 이 유용할 수 있는 상황과 그렇지 않은 상황에 대해서도 알아봅니다. 

## Word piece, units of words

Word Piece Model 은 제한적인 vocabulary, 정확히는 단어를 표현할 수 있는 subwords unit 으로 모든 단어를 표현하기 위한 방법입니다. 텍스트 마이닝을 할 때 수십, 수백만개의 단어가 등장하기 때문에 bag of words model 에서는 단어 개수 만큼의 차원을 지닌 벡터 공간을 이용합니다. RNN 처럼 word embedding vectors 를 이용하는 모델은 각각의 단어에 대한 embedding vector 를 이용합니다. 단어의 개수가 많을수록 차원이 크거나 모델이 무거워집니다. 이를 해결하기 위해 제한된 (finite) 개수의 단어 유닛으로 모든 단어를 표현하는 토크나이저가 제안되었습니다. 

언어는 글자 (characters)를 subword units 으로 이용합니다. 영어는 알파벳을 유닛으로 이용합니다. 하지만 대부분의 영어 단어는 몇 개의 글자가 모여 하나의 단어를 구성합니다. 그렇기 때문에 하나의 유닛이 어떤 개념을 지칭하기는 어렵습니다. 유닛이 모호성을 지닙니다. 중국어는 한자를 units 으로 이용합니다. 물론 중국어도 여러 글자가 모여 하나의 단어를 이루기도 하지만, 한 글자로 구성된 단어도 많습니다. 영어보다는 유닛의 모호성이 줄어듭니다. 동음이의어의 문제가 남아있지만, 가장 모호성이 적은 방법은 모든 단어를 단어의 유닛으로 이용하는 것입니다. 명확합니다. 

그러나 토크나이징 방법에 따라 모호성이 적은 최소한의 유닛을 만들 수도 있습니다. 아래의 세 문장을 다음의 요소들로 나눈다면 이 유닛들은 의미를 보존하면서도 재활용이 될 수 있습니다. 

	공연은 끝났어 -> ['공연-' + '-은' + '끝-' + '-났어']
	공연을 끝냈어 -> ['공연-' + '-을' + '끝-' + '-냈어']
	개막을 해냈어 -> ['개막-' + '-을' + '해-' + '-냈어']

이 유닛들은 '개막공연' 이라는 복합명사도 분해하는데 이용될 수 있습니다. '개막공연'을 독립된 유닛으로 만들 필요가 없습니다.

	개막공연을 끝냈어 -> ['개막-' + '공연-' + '-을' + '끝-' + '-냈어']

그런데 문제는 위처럼 토크나이징을 하려면 해당 언어의 언어학적 지식과 학습데이터가 필요합니다. 그러나 언어가 다르고, 도메인이 다르면 이를 준비하는 것은 어렵습니다. 

## Word Piece Model (sentencepiece) tokenizer

하지만 학습 데이터를 이용하지 않으면서도 위의 결과를 이끌 수 있는 heuristics 이 있습니다. '공연-', '개막-', '-냈어', '-났어'은 유닛이기 때문에 유닛이 아닌 subwords 보다 자주 등장할 가능성이 높습니다. 만약 '공연-'의 빈도수와 '개막공연-'의 빈도수가 같다면 '공연-'이나 '개막-'은 유닛으로 이용하지 않아도 됩니다. 어자피 세 유닛 모두 '개막공연'을 나타내기 위한 부분들이니까요. 유닛이 자주 등장한다는 사실은 아마도 많은 언어의 공통적인 특징일 것입니다. Language independent, universial tokenizer 를 만들 수도 있을 것 같습니다.

Word Piece Model (WPM) 은 이 개념을 이용하는 토크나이저입니다. 원 논문 (Sennrich et al., 2015) 에 적힌 예시입니다. Word 는 매우 다양합니다. makers, over 는 모두 자주 이용되기 때문에 그 자체를 units 으로 이용합니다. 하지만 Jet 은 자주 등장하지 않는 단어이기 때문에 subword units 인 'J' 와 'et' 으로 나눕니다. 

그 전에 모든 단어의 시작에는 underbar, '_' 를 붙입니다. 그렇다면 'Jet' 은 '_Jet' 이 되어 '_J et' 으로 나뉘어집니다. makers 는 '_makers' 가 됩니다. 

- **Word**      : Jet makers feud over seat width with big orders at stake
- **Wordpieces**: _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake

이는 이후 문장 생성, 혹은 subwords 부터의 문장 복원을 위한 것입니다. Underbar 없이 subwords 를 띄어두면, 본래 띄어쓰기와 구분이 되지 않기 때문입니다. 문장을 복원하는 코드는 간단합니다. 띄어쓰기 기준으로 나눠진 tokens 을 concatenation 한 뒤, _로 다시 나눠 tokenize 할 수도 있으며, _ 를 빈 칸으로 치환합니다.

{% highlight python %}
def recover(tokens):
    sent = ''.join(tokens)
    sent = sent.replace('_', ' ')
    return sent
{% endhighlight %}

## Byte-pair Encoding (BPE)

Neural Machine Translation of Rare Words with Subword Units
 (Sennrich et al., 2015) 에는 word pieces (subword units) 을 학습할 수 있는 간단한 코드가 적혀 있습니다. 

처음에는 모든 단어를 글자로 나눕니다. Character 는 기본 subword units 입니다. 그 다음에는 가장 빈도수가 많은 bigram 을 찾습니다. 이 bigram 을 하나의 unit 으로 merge 합니다. 이 과정을 num_merges 만큼 반복합니다. vocab 의 value 는 빈도수 입니다. 'low' 가 5 번, 'lower' 가 2 번 등장했습니다. 

{% highlight python %}
import re, collections

def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

vocab = {'l o w </w>' : 5,
         'l o w e r </w>' : 2,
         'n e w e s t </w>':6,
         'w i d e s t </w>':3
         }

num_merges = 10

for i in range(num_merges):
    pairs = get_stats(vocab)
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(best)

print(vocab)
{% endhighlight %}

위 코드를 실행시킬 때 print(best) 에 의하여 출력된 결과입니다. 처음 'es' 는 9 번 등장하였기 때문에 ('e', 's') 가 'es' 로 합쳐집니다. 'lo' 는 7 번 등장했습니다만 Python dict 의 key order 순서에 의해 'es' 가 우선적으로 merge 됩니다. 그리고 ('es', 't') 가 병합되어 'est'가 됩니다. 같은 빈도수를 지닌다면 계속하여 병합이 됩니다. 하지만 'west': 6 번, 'dest': 3 번 이기 때문에 'est' 이후에는 ('l', 'o'), ('lo', 'w') 가 병합됩니다. 

	('e', 's')
	('es', 't')
	('est', '</w>')
	('l', 'o')
	('lo', 'w')
	('n', 'e')
	('ne', 'w')
	('new', 'est</w>')
	('low', '</w>')
	('w', 'i')

10 번의 병합을 거친 vocab 은 다음처럼 변합니다. 

	vocab = {
		     'low</w>': 5, 
		     'low e r </w>': 2, 
		     'newest</w>': 6, 
		     'wi d est</w>': 3
		    }

띄어쓰기 기준으로 subword units 를 나누면 다음과 같습니다. 총 9 개의 units 으로 네 단어를 표현합니다.

	{'low</w>': 5,
	 'low': 2,
	 'e': 2,
	 'r': 2,
	 '</w>': 2,
	 'newest</w>': 6,
	 'wi': 3,
	 'd': 3,
	 'est</w>': 3
	}

Byte pair encoding 은 데이터 압축 방법입니다. 빈도수가 많은 최장길이의 substring 을 하나의 unit 으로 만들면 bit 를 아낄 수 있습니다. 

토크나이저 입장에서는 많이 쓰이는 subwords 를 units 으로 이용하면, 자주 이용되는 단어는 그 자체가 unit 이 되며, 자주 등장하지 않는 단어 (rare words)가 subword units 으로 나뉘어집니다. 

그리고 번역 엔진들에서는 자주 이용되는 $$k_1$$ 개의 단어는 따로 지정을 하고, 그 외의 단어에 대해서 $$k_2$$ 개의 subword units 을 이용하여 토크나이징을 수행한다고 합니다.  

논문의 원저자인 Sennrich 도 본인의 [github][rsennrich_bpe] 에 코드를 공개하였습니다. Python script 형식으로 input file 을 토크나이징하여 output file 로 만듭니다. 

## Codes

Sennrich et al (2015) 에 공개된 코드에 save, load 와 같은 기능을 추가하여 간단히 word piece model 을 구현하였습니다. 코드는 [github][lovit_wpm]에 공개하였습니다. 

BytePairEncoder(n_units) 은 n_units 개수 만큼의 subword units 을 학습하는 클래스입니다. corpus 는 list of str (like) 입니다. 

{% highlight python %}
from bytepairencoder import BytePairEncoder

n_units = 5000
encoder = BytePairEncoder(n_units)
encoder.train(corpus)
{% endhighlight %}

학습이 끝낸 encoder의 tokenize() 를 이용하여 토크나이징을 할 수 있습니다. 

{% highlight python %}
tokens = encoder.tokenize(sent)
{% endhighlight %}

save 와 load 는 다음과 같습니다. model_path 를 입력합니다. 

{% highlight python %}
encoder.save(model_path)

loaded_encoder = BytePairEncoder()
loaded_encoder.load(model_path)
{% endhighlight %}

2016-10-20 의 뉴스를 이용하여 5,000 개의 subword units 을 학습한 뒤, 토크나이징을 하였습니다. 

{% highlight python %}
sent = '오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다'

encoder.tokenize(sent)
{% endhighlight %}

오패산터널의 경우, 잘 등장하지 않은 단어이기 때문에 모든 단어가 글자들로 나뉘어 집니다. '용의자'는 빈도수 상위 5000 등 안에 들지 못하였습니다. 하지만 '의자'는 5000 등 안에 들었기 때문에 하나의 subword unit 이 되었습니다. '연합뉴스'는 뉴스 문서에서는 매우 빈번한 단어이기 때문에 하나의 단어임을 확인할 수 있습니다. 

	오 패 산 터 널_ 총 격 전_ 용 의자 _ 검 거_ 서울_ 연합뉴스_ 경찰_ 관계 자들이_ 19일_ 오후_ 서울_ 강 북 구_ 오 패 산_ 터 널_ 인근 에서_ 사제 _ 총 기를_ 발사 해_ 경찰 을_ 살 해 한_ 용 의자 _ 성 모 씨를_ 검 거 하고_ 있다_ 성 씨는_ 검 거_ 당시_ 서 바이 벌_ 게임 에서_ 쓰 는_ 방탄 조 끼 에_ 헬 멧 까지_ 착 용한_ 상태 였다_

의미적으로 '용' + '의자' = '용의자'는 아닙니다. 하지만 composition 을 통하여 '용의자'의 의미를 파악하는 것은 더 이상 토크나이저의 몫은 아닙니다. 이 부분은 번역기, 문서 판별기 등의 알고리즘의 몫입니다. 

## Sentiment classification

네이버 영화 댓글과 평점을 이용하여 영화 평의 긍/부정을 구분하는 감성 분석을 수행하였습니다. 

영화평의 감성 분석을 위해서는 데이터 전처리가 필요합니다. 영화 평점은 1 ~ 10 점을 지닙니다. 사람마다 점수의 기준이 다르기 때문에 4 ~ 7 점은 긍/부정을 명확히 판단하기 어려워 제외하였습니다. 또한 1 ~ 3 점을 서로 다른 클래스로 구분하는 것도 큰 의미가 없습니다. 그렇기 때문에 1 ~ 3 점은 negative, 8 ~ 10 점은 positive 로 레이블을 부여하였습니다. 

객관적인 성능의 비교를 위하여 트위터 한국어 분석기를 함께 이용하였습니다. 문서 분류, 특히 감성 분석에는 bigram 이 유용합니다. 그렇기 때문에 unigram 만 이용한 경우와 uni, bigram 을 함께 이용한 성능을 측정하였습니다. WPM 옆의 숫자는 units 의 개수입니다.

| Tokenizer | unigram accuracy | uni + bigram accuracy |
| --- | --- | --- |
| WPM 3000 | 89.12 % | 92.67 % |
| WPM 5000 | 89.56 % | 92.95 % |
| WPM 10000 | 91.69 % | 93.47 % |
| WPM 20000 | 92.23 % | 93.41 % |
| WPM 30000 | 92.43 % | 93.35 % |
| WPM 50000 | 92.65 % | 93.32 % |
| 트위터 한국어 분석기 | 91.91 % | 93.39 % |

트위터 한국어 분석기는 unigram 만으로도 91.9 % 의 성능을 보입니다. Word Piece Model 의 unigram 의 경우, unit 이 많을수록 성능이 올라갑니다. 20K 의 units 을 이용하면 오히려 트위터 한국어 분석기보다도 좋은 성능을 보입니다. 이는 빈번하게 등장하면서도 긍/부정을 판단하는데 유용한 subwords 가 존재하는데, 트위터 한국어 분석기는 이를 형태소들로 분해하였다는 의미입니다. 

Word Piece Model 도 bigram 을 함께 이용하면 성능이 올라갑니다. 감성 분석은 bigram 이 중요한 features 입니다. '재미'라는 단어는 긍/부정을 판단하기 어렵습니다. 뒤에 따라오는 단어와 함께 '재미 + 없다' / '재미 + 있다' 처럼 bigram 이 이뤄져야 유의미한 features 가 됩니다. 트위터 한국어 분석기는 '재미없다 -> 재미 + 없다'로 나눕니다. 그렇기 때문에 bigram 을 이용하면 성능이 증가합니다. 

그러나 Word Piece Model 을 이용하여도 성능의 상한선이 존재합니다. 10K 과 bigram 을 이용할 때 최고의 정확도인 93.47 % 을 보이며, 그 이후에는 bigram 을 이용하여도 성능이 내려갑니다. 과적합 (over-fitting) 의 문제라고 예상됩니다. 

또한 50K 의 유닛을 이용할 때와 3K 의 유닛과 bigram 을 이용할 때의 성능이 92.65 % 와 92.67 % 로 비슷합니다. 실험을 수행할 때, 빈도수 20 이하의 uni, bigram 은 모두 제거를 하였습니다. 그 결과 3K 의 유닛의 uni + bigram 의 개수는 56K 입니다. Bigram 으로 선택된 features 가 50K units 을 이용하는 WPM 의 unigram 과 비슷하다는 의미입니다. 즉, units 의 개수를 늘리면 bigram 으로 만들어져야 할 features 가 unigram 으로 만들어지는 효과를 얻습니다. 

## References
- [Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.][subword_mt]
- [Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... & Klingner, J. (2016). Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.][gnmt]

[google_wpm]: https://github.com/google/sentencepiece
[gnmt]: https://arxiv.org/abs/1609.08144
[lovit_wpm]: https://github.com/lovit/wordpiecemodel
[rsennrich_bpe]: https://github.com/rsennrich/subword-nmt
[subword_mt]:https://arxiv.org/abs/1508.07909