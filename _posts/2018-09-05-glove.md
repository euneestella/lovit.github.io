---
title: GloVe, word representation
date: 2018-09-05 21:00:00
categories:
- nlp
- representation
tags:
- word representation
---

## Brief reviews of Word2Vec

Word2Vec 은 Softmax regression 을 이용하여 단어의 의미적 유사성을 보존하는 embedding space 를 학습합니다.
문맥이 유사한 (의미가 비슷한) 단어는 비슷한 벡터로 표현됩니다.
'cat' 과 'dog' 의 벡터는 매우 높은 cosine similarity 를 지니지만, 이들은 'topic modeling' 의 벡터와는 매우 작은 cosine similarity 를 지닙니다.

Word2Vec 은 softmax regression 을 이용하여 문장의 한 스냅샷에서 기준 단어의 앞/뒤에 등장하는 다른 단어들 (context words) 이 기준 단어를 예측하도록 classifier 를 학습합니다.
그 과정에서 단어의 embedding vectors 가 학습됩니다.
Context vector 는 앞/뒤 단어들의 평균 임베딩 벡터 입니다.
[a, little, cat, sit, on, the, table] 문장에서 context words [a, little, sit, on] 를 이용하여 cat 을 예측합니다.

![]({{ "/assets/figures/word2vec_logistic_structure.png" | absolute_url }})

이는 cat 의 임베딩 벡터를 context words 의 평균 임베딩 벡터에 가깝도록 이동시키는 역할을 합니다. 비슷한 문맥을 지니는 dog 도 비슷한 context words 의 평균 임베딩 벡터에 가까워지기 때문에 cat 과 dog 의 벡터가 비슷해집니다. 

![]({{ "/assets/figures/word2vec_softmax.png" | absolute_url }})



이외의 자세한 Word2Vec 의 설명은 [이전 포스트][word2vec_post]를 참고하세요. 

## GloVe

![]({{ "/assets/figures/glove-weighting-function.png" | absolute_url }}){: width="50%" height="50%"}


## Python package

{% highlight python %}
{% endhighlight %}


{% highlight python %}
{% endhighlight %}


{% highlight python %}
{% endhighlight %}


{% highlight python %}
{% endhighlight %}


{% highlight python %}
{% endhighlight %}

{% highlight python %}
{% endhighlight %}

{% highlight python %}
{% endhighlight %}

## Reference

- Pennington, J., Socher, R., & Manning, C. (2014). [Glove: Global vectors for word representation.][glove_paper] In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).


[word2vec_post]: {{ site.baseurl }}{% link _posts/2018-03-26-word_doc_embedding.md %}
[glove_paper]: https://nlp.stanford.edu/projects/glove/
[glove_git]: https://github.com/maciejkula/glove-python