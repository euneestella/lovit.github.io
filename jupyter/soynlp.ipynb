{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '/mnt/lovit/works/fastcampus_text_ml/1st/data/corpus_10days/news/2016-10-20_article_all_normed.txt'\n",
    "\n",
    "import sys\n",
    "sys.path.append('/mnt/lovit/git/soynlp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223357\n"
     ]
    }
   ],
   "source": [
    "from soynlp import DoublespaceLineCorpus\n",
    "\n",
    "corpus = DoublespaceLineCorpus(corpus_path, iter_sent=True)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.691 Gbse memory 0.744 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 360721\n",
      "all accessor variety was computed # words = 360721\n",
      "CPU times: user 54.7 s, sys: 260 ms, total: 54.9 s\n",
      "Wall time: 54.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "word_extractor = WordExtractor(min_count=5)\n",
    "word_extractor.train(corpus)\n",
    "word_scores = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scores(cohesion_forward=0.1943363253634125, cohesion_backward=0.07681159420289856, left_branching_entropy=3.204801502248508, right_branching_entropy=0.42721236711742844, left_accessor_variety=154, right_accessor_variety=42, leftside_frequency=4893, rightside_frequency=159)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word = 연합, lav = 154, rav=42\n",
      "word = 연합뉴, lav = 0, rav=0\n",
      "word = 연합뉴스, lav = 138, rav=158\n"
     ]
    }
   ],
   "source": [
    "for word in '연합 연합뉴 연합뉴스'.split():\n",
    "    if word in word_scores:\n",
    "        score = word_scores[word]\n",
    "        (lav, rav) = (score.left_accessor_variety, score.right_accessor_variety)\n",
    "    else:\n",
    "        (lav, rav) = (0.0, 0.0)\n",
    "    print('word = %s, lav = %d, rav=%d' % (word, lav, rav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word = 연합  , lbe = 3.2, rbe=0.427\n",
      "word = 연합뉴 , lbe = 0.0, rbe=0.0\n",
      "word = 연합뉴스, lbe = 3.02, rbe=3.9\n"
     ]
    }
   ],
   "source": [
    "for word in '연합 연합뉴 연합뉴스'.split():\n",
    "    if word in word_scores:\n",
    "        score = word_scores[word]\n",
    "        (lbe, rbe) = (score.left_branching_entropy, score.right_branching_entropy)\n",
    "    else:\n",
    "        (lbe, rbe) = (0.0, 0.0)\n",
    "    print('word = {:4}, lbe = {:.3}, rbe={:.3}'.format(word, lbe, rbe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cohesion_scores = {word:score.cohesion_forward for word, score in word_scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아이오아이', '의', '무대', '가', '방송', '에', '중계', '되었습니다']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "from soynlp.tokenizer import RegexTokenizer\n",
    "\n",
    "ltokenizer = LTokenizer(scores = cohesion_scores)\n",
    "ltokenizer.tokenize('아이오아이의 무대가 방송에 중계되었습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아이오아이', '의'), ('무대', '가'), ('방송', '에'), ('중계', '되었습니다')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('아이오아이의 무대가 방송에 중계되었습니다', flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아이오아이', '무대', '방송', '중계']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltokenizer.tokenize('아이오아이의 무대가 방송에 중계되었습니다', remove_r=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아이오아이', '의', '무대', '가', '방송', '에', '중계', '되었습니다']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxscoretokenizer = MaxScoreTokenizer(scores = cohesion_scores)\n",
    "maxscoretokenizer.tokenize('아이오아이의무대가방송에중계되었습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('아이오아이', 0, 5, 0.30063636035733476, 5),\n",
       "  ('의', 5, 6, 0.0, 1),\n",
       "  ('무대', 6, 8, 0.042336645588678112, 2),\n",
       "  ('가', 8, 9, 0.0, 1),\n",
       "  ('방송', 9, 11, 0.31949135704351284, 2),\n",
       "  ('에', 11, 12, 0.0, 1),\n",
       "  ('중계', 12, 14, 0.0019356503785271852, 2),\n",
       "  ('되었습니다', 14, 19, 0.2762976357271788, 5)]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxscoretokenizer.tokenize('아이오아이의무대가방송에중계되었습니다', flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 s, sys: 144 ms, total: 16.8 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = [ltokenizer.tokenize(sent) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('로딩', 'Noun'),\n",
       " ('시간', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('제외하기', 'Verb'),\n",
       " ('위한', 'Verb'),\n",
       " ('예문', 'Noun')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "twitter.pos('로딩시간을제외하기위한예문')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 29s, sys: 1.69 s, total: 4min 31s\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "twitter_words = [twitter.pos(sent) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_words = [['%s/%s' % (w,t) for w,t in sent] for sent in twitter_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(count:5616, index:76, sample_int:4294967296)\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.wv.vocab['방송'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('예능프로그램', 0.6981078386306763),\n",
       " ('예능', 0.688145637512207),\n",
       " ('방영', 0.660905659198761),\n",
       " ('라디오스타', 0.6552329659461975),\n",
       " ('라디오', 0.6498782634735107),\n",
       " ('엠카운트다운', 0.6143872141838074),\n",
       " ('파워타임', 0.586585521697998),\n",
       " ('식사하셨어요', 0.5865078568458557),\n",
       " ('한끼줍쇼', 0.5841958522796631),\n",
       " ('황금어장', 0.5715700387954712)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('방송')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('연예', 0.6419150233268738),\n",
       " ('화면', 0.6120390892028809),\n",
       " ('뉴스부장', 0.5935080051422119),\n",
       " ('뉴스부', 0.5910232067108154),\n",
       " ('라디오', 0.5880887508392334),\n",
       " ('채널', 0.5730870962142944),\n",
       " ('정보팀', 0.5722399950027466),\n",
       " ('속보팀', 0.5669897794723511),\n",
       " ('앵커', 0.5661150217056274),\n",
       " ('토크쇼', 0.5654663443565369)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('뉴스')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('작품', 0.7261432409286499),\n",
       " ('드라마', 0.7252553701400757),\n",
       " ('흥행', 0.6903091669082642),\n",
       " ('독립영화', 0.6875913143157959),\n",
       " ('걷기왕', 0.654923677444458),\n",
       " ('감독', 0.6543536186218262),\n",
       " ('럭키', 0.6453847885131836),\n",
       " ('다큐멘터리', 0.6425734758377075),\n",
       " ('블록버스터', 0.6288458704948425),\n",
       " ('주연', 0.6285911798477173)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('영화')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('타이틀곡', 0.8425650000572205),\n",
       " ('에이핑크', 0.8301823139190674),\n",
       " ('샤이니', 0.8209962844848633),\n",
       " ('너무너무', 0.8158787488937378),\n",
       " ('다이아', 0.8083204030990601),\n",
       " ('트와이스', 0.8079981803894043),\n",
       " ('파이터', 0.8070152997970581),\n",
       " ('잠깐만', 0.7995353937149048),\n",
       " ('몬스', 0.7901242971420288),\n",
       " ('불독의', 0.7892482280731201)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('아이오아이')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_twitter = Word2Vec(twitter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('드라마/Noun', 0.6787333488464355),\n",
       " ('작품/Noun', 0.677376389503479),\n",
       " ('독립영화/Noun', 0.6677168607711792),\n",
       " ('영화로/Noun', 0.6049726009368896),\n",
       " ('주연/Noun', 0.5995980501174927),\n",
       " ('럭키/Noun', 0.5985063314437866),\n",
       " ('감독/Noun', 0.5851325988769531),\n",
       " ('다큐멘터리/Noun', 0.5788881778717041),\n",
       " ('청춘/Noun', 0.5751833915710449),\n",
       " ('강동원/Noun', 0.5721665620803833)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_twitter.most_similar('영화/Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('연합뉴스/Noun', 0.6110204458236694),\n",
       " ('연예/Noun', 0.5120304822921753),\n",
       " ('기자/Noun', 0.4866550862789154),\n",
       " ('김도연/Noun', 0.48150816559791565),\n",
       " ('렬/Noun', 0.47369953989982605),\n",
       " ('권현진/Noun', 0.47151216864585876),\n",
       " ('방지영/Noun', 0.46529433131217957),\n",
       " ('2580/Number', 0.46096765995025635),\n",
       " ('한대욱/Noun', 0.4602851867675781),\n",
       " ('김영선/Noun', 0.4595600962638855)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_twitter.most_similar('뉴스/Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('방영/Noun', 0.6817507147789001),\n",
       " ('엠카운트다운/Noun', 0.6040224432945251),\n",
       " ('예능/Noun', 0.5873677134513855),\n",
       " ('라디오스타/Noun', 0.5550553202629089),\n",
       " ('방송사/Noun', 0.5497354865074158),\n",
       " ('첫방송/Noun', 0.5472666621208191),\n",
       " ('방송한/Verb', 0.5442488789558411),\n",
       " ('수목드라마/Noun', 0.5297601222991943),\n",
       " ('라디오/Noun', 0.5036641955375671),\n",
       " ('준영/Noun', 0.49247831106185913)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_twitter.most_similar('방송/Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '아이오아이/Noun' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-63b663fe377f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2vec_twitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'아이오아이/Noun'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/lovit/anaconda2/envs/scrapper/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \"\"\"\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lovit/anaconda2/envs/scrapper/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lovit/anaconda2/envs/scrapper/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '아이오아이/Noun' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "word2vec_twitter.most_similar('아이오아이/Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
