{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from navermovie_comments import load_trained_embedding\n",
    "from navermovie_comments import load_movie_comments\n",
    "\n",
    "word2vec_model = load_trained_embedding(\n",
    "    data_name = 'large',\n",
    "    tokenize = 'soynlp_unsup',\n",
    "    embedding = 'word2vec'\n",
    ")\n",
    "\n",
    "_, texts, _ = load_movie_comments(\n",
    "    large = True,\n",
    "    tokenize = 'soynlp_unsup'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93234, 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_vocab = word2vec_model.wv.index2word\n",
    "wv = word2vec_model.wv.vectors\n",
    "\n",
    "wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93235, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "wv_ = np.vstack([wv, np.zeros((1, wv.shape[1]), dtype=wv.dtype)])\n",
    "wv_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovit/anaconda3/envs/pytorch/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "seed_words = {word for word, _ in word2vec_model.wv.most_similar('송강호', topn=100)}\n",
    "seed_words.update({word for word, _ in word2vec_model.wv.most_similar('디카프리오', topn=100)})\n",
    "\n",
    "print(len(seed_words)) # 172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(362379)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset(idx_to_vocab, texts, seed_words, window=2):\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    padding_idx = len(vocab_to_idx)\n",
    "\n",
    "    def encode(words):\n",
    "        encode_ = lambda vocab:vocab_to_idx.get(vocab, padding_idx)\n",
    "        return [encode_(w) for w in words]\n",
    "\n",
    "    def slice_pad(idxs, i, n):\n",
    "        left = idxs[max(0, i-window):i]\n",
    "        if len(left) < window:\n",
    "            left = [padding_idx] * (window - len(left)) + left\n",
    "        right = idxs[i+1:min(i+1+window, n)]\n",
    "        if len(right) < window:\n",
    "            right = right + [padding_idx] * (window - len(right))\n",
    "        return left + right\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        n = len(words)\n",
    "        labels = [1 if w in seed_words else 0 for w in words]\n",
    "        idxs = encode(words)\n",
    "        for i, label in enumerate(labels):\n",
    "            context = slice_pad(idxs, i, n)\n",
    "            x.append(np.asarray(context))\n",
    "            y.append(label)\n",
    "\n",
    "    x = np.vstack(x)\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "    return x, y\n",
    "\n",
    "x, y = create_dataset(idx_to_vocab, texts, seed_words)\n",
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERWindowDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "ner_dataloader = DataLoader(\n",
    "    NERWindowDataset(x, y),\n",
    "    batch_size = 64,\n",
    "    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for input_, output_ in ner_dataloader:\n",
    "    print(input_.size())\n",
    "    print(output_.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedEntityWindowClassifier(nn.Module):\n",
    "    def __init__(self, wordvec, n_classes, hidden_1_dim=50, n_windows=2):\n",
    "        super(NamedEntityWindowClassifier, self).__init__()\n",
    "\n",
    "        self.n_windows = n_windows\n",
    "        self.n_vocabs, self.embed_dim = wordvec.shape\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings = self.n_vocabs,\n",
    "            embedding_dim = self.embed_dim\n",
    "        )\n",
    "        self.fc1 = nn.Linear(\n",
    "            self.embed_dim * 2 * n_windows,\n",
    "            hidden_1_dim,\n",
    "            bias = False\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            hidden_1_dim,\n",
    "            n_classes,\n",
    "            bias = True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.LongTensor\n",
    "            context word index. size is 2 * n_windows\n",
    "        \"\"\"\n",
    "        y = self.embed(x) # [batch, 2 * window, embed]\n",
    "        y = y.view(y.size()[0], -1) # [batch, embed * 2 * widow]\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = self.fc2(y)\n",
    "        return y\n",
    "\n",
    "model = NamedEntityWindowClassifier(wv_, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, loss_func, optimizer, epochs):\n",
    "    n_batchs = len(ner_dataloader)\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_func(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.data.numpy()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                loss_tmp = loss_sum / (i+1)\n",
    "                template = '\\repoch = {}, batch = {} / {}, training loss = {}'\n",
    "                message = template.format(epoch, i, n_batchs, '%.3f' % loss_tmp)\n",
    "                print(message, end='')\n",
    "\n",
    "        print('\\r## epoch = {}, training loss = {}'.format(epoch, '%.3f' % (loss_sum / (i+1)) ))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, batch = 12200 / 20510, training loss = 0.047"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5b97edb8d01a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-202ca4f0f970>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_loader, model, loss_func, optimizer, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameter for the optimizer\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "train(ner_dataloader, model, loss_func, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[6.8958e-02, 7.4167e-01, 4.8499e-01, 6.2312e-01],\n",
       "         [4.1036e-02, 4.2841e-01, 3.4260e-01, 3.8961e-01],\n",
       "         [3.1294e-04, 4.9025e-01, 1.0517e-01, 9.9408e-01]],\n",
       "\n",
       "        [[7.7422e-01, 2.0903e-01, 5.2505e-01, 1.3969e-01],\n",
       "         [4.4244e-01, 3.0134e-01, 4.0958e-01, 7.1006e-01],\n",
       "         [8.1710e-01, 1.5755e-01, 2.8302e-01, 2.5122e-01]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor(np.random.random_sample((2, 3, 4)))\n",
    "print(z.size())\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.8958e-02, 7.4167e-01, 4.8499e-01, 6.2312e-01, 4.1036e-02, 4.2841e-01,\n",
       "         3.4260e-01, 3.8961e-01, 3.1294e-04, 4.9025e-01, 1.0517e-01, 9.9408e-01],\n",
       "        [7.7422e-01, 2.0903e-01, 5.2505e-01, 1.3969e-01, 4.4244e-01, 3.0134e-01,\n",
       "         4.0958e-01, 7.1006e-01, 8.1710e-01, 1.5755e-01, 2.8302e-01, 2.5122e-01]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.view(z.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
